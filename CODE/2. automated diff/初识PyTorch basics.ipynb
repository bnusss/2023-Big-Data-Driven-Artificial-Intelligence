{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一课 当深度学习遇上PyTorch\n",
    "\n",
    "在这节课中，我们主要展示了PyTorch的使用方法，以及如何用PyTorch实现一个线性回归算法\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、有关Tensor和Autograd变量的练习\n",
    "### 1. Tensor\n",
    "#### a. 产生Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  #导入torch包\n",
    "torch.__version__ #显示当前PyTorch版本号，笔者用的是0.1.12_2，有些命令可能在新的版本下无法执行，请参考PyTorch文件找到最新的相应命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)  #产生一个5*3的tensor，随机取值\n",
    "x  #显示x的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(5, 3) #产生一个5*3的Tensor，元素都是1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(2, 5, 3) #产生一个5*3的Tensor，元素都是1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.FloatTensor([[0.3297,0.7021,0.1119],[0.6668,0.6904,0.1953],[0.6683,0.4260,0.2950],[0.0899,0.4099,0.0882],[0.4675,0.8369,0.1926]])\n",
    "z = x + y #两个tensor可以直接相加\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的语句展示了两个tensor按照矩阵的方式相乘，注意x的尺寸是5*3，y的尺寸也是5*3无法进行矩阵乘法，所以先将y进行转置。\n",
    "转置操作可以用.t()来完成，也可以用<!-- lang:python-->.transpose(0, 1)来完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = x.mm(y.t()) #x乘以y的转置\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Tensor与numpy.ndarray之间的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #导入numpy包\n",
    "a = np.ones([5, 3]) #建立一个5*3全是1的二维数组（矩阵）\n",
    "b = torch.from_numpy(a) #利用from_numpy将其转换为tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.FloatTensor(a) #另外一种转换为tensor的方法，类型为FloatTensor，还可以使LongTensor，整型数据类型\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()  #从一个tensor转化为numpy的多维数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor和numpy的最大区别在于tensor可以在GPU上运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  #检测本机器上有无GPU可用\n",
    "    x = x.cuda() #返回x的GPU上运算的版本\n",
    "    y = y.cuda()\n",
    "    print(x + y) #tensor可以在GPU上正常运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  #检测本机器上有无GPU可用\n",
    "    device = torch.device(\"cuda\")          # 选择一个CUDA设备\n",
    "    y = torch.ones_like(x, device=device)  # 在GPU上直接创建张量\n",
    "    x = x.to(device)                       # 也可以直接加载``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # 转回到CPU上``.to``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 有关自动微分变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((2, 2), requires_grad=True)  \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2  #可以按照Tensor的方式进行计算\n",
    "y.grad_fn  \n",
    "#注：在新版本PyTorch中，可以用.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y  #可以进行各种符合运算\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.mean(y * y)  #也可以进行复合运算\n",
    "z.data #.data属性可以返回z所包裹的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** backward可以实施反向传播算法，并计算所有计算图上叶子节点的导数（梯度）信息。注意，由于z和y都不是叶子节点，所以都没有梯度信息）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() #梯度反向传播\n",
    "print(z.grad)\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的例子中，我们让矩阵x反复作用在向量x上，系统会自动记录中间的依赖关系和长路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor([[0.01, 0.02]], requires_grad = True) #创建一个1*2的tensor（1维向量）\n",
    "x = torch.ones(2, 2, requires_grad = True) #创建一个2*2的矩阵型tensor\n",
    "for i in range(10):\n",
    "    s = s.mm(x)  #反复用s乘以x（矩阵乘法），注意s始终是1*2的tensor\n",
    "z = torch.mean(s) #对s中的各个元素求均值，得到一个1*1的scalar（标量，即1*1张量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() #在具有很长的依赖路径的计算图上用反向传播算法计算叶节点的梯度\n",
    "print(x.grad)  #x作为叶节点可以获得这部分梯度信息\n",
    "print(s.grad)  #s不是叶节点，没有梯度信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 一个动态生成的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(1,10), requires_grad=True)\n",
    "prev_h = Variable( torch.randn(1,20), requires_grad=True)\n",
    "W_h = Variable( torch.randn(20,20), requires_grad=True)\n",
    "W_x = Variable( torch.randn(20,10), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2h = torch.mm(W_x,x.t())\n",
    "h2h = torch.mm(W_h,prev_h.t())\n",
    "next_h = i2h + h2h\n",
    "next_h = next_h.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_h.backward(torch.ones(20,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、利用PyTorch实现简单的线性回归算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 准备数据\n",
    "\n",
    "在这里，我们人为生成一些样本点作为我们的原始数据\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 100, 100).type(torch.FloatTensor) #linspace可以生成0-100之间的均匀的100个数字\n",
    "rand = torch.randn(100) * 10 #随机生成100个满足标准正态分布的随机数，均值为0，方差为1.将这个数字乘以10，标准方差变为10\n",
    "y = x + rand #将x和rand相加，得到伪造的标签数据y。所以(x,y)应能近似地落在y=x这条直线上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[: -10]\n",
    "x_test = x[-10 :]\n",
    "y_train = y[: -10]\n",
    "y_test = y[-10 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将生成的训练数据点画在图上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #导入画图的程序包\n",
    "plt.figure(figsize=(10,8)) #设定绘制窗口大小为10*8 inch\n",
    "plt.plot(x_train.numpy(), y_train.numpy(), 'o') \n",
    "plt.xlabel('X') #添加X轴的标注\n",
    "plt.ylabel('Y') #添加Y周的标注\n",
    "plt.show() #将图形画在下面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构造模型，计算损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在pytorch已支持自动尺寸调整，通过自动广播来调整张量的大小，不需要在程序代码中调用expand_as方法。\n",
    "\n",
    "expand_as方法的原理如下。首先，a的维度为1，x的维度为100*1的Tensor，这两者不能直接相乘，因为维度不同。\n",
    "\n",
    "所以，先要将a升维成1*1的Tensor。这就好比将原本在直线上的点被升维到了二维平面上，同时直线仍然在二维平面中。\n",
    "\n",
    "```expand_as(x)```可以将张量升维成与x同维度的张量。所以如果a = 1, x为尺寸为100，那么，\n",
    "\n",
    "a.expand_as(x)$ = (1, 1, \\cdot\\cdot\\cdot, 1)^T$\n",
    "\n",
    "```x * y```为两个1维张量的乘积，计算结果：\n",
    "\n",
    "$(x * y)_i = x_i \\cdot y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, requires_grad = True)\n",
    "b = torch.rand(1, requires_grad = True)\n",
    "predictions = a * x_train + b\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((predictions - y_train) ** 2)  #计算损失函数\n",
    "loss.backward() #开始反向传播梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始梯度下降，其中0.001为学习率\n",
    "a.data.add_(- 0.001 * a.grad.data) \n",
    "b.data.add_(- 0.001 * b.grad.data)\n",
    "\n",
    "#注意我们无法改变一个tensor，而只能对tensor的data属性做更改\n",
    "#所有函数加“_”都意味着需要更新调用者的数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练模型的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 错误版本\n",
    "\n",
    "错误在于，每一步迭代周期没有将a和b的梯度（grad）数值设置为0，导致每一步backward候梯度就会不断累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, requires_grad = True)\n",
    "b = torch.rand(1, requires_grad = True)\n",
    "print('Initial parameters:', a, b)\n",
    "learning_rate = 0.0001\n",
    "for i in range(1000):\n",
    "    predictions = a * x_train + b\n",
    "    loss = torch.mean((predictions - y_train) ** 2)\n",
    "    print('loss:', loss)\n",
    "    loss.backward()\n",
    "    a.data.add_(- learning_rate * a.grad.data)\n",
    "    b.data.add_(- learning_rate * b.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印输出的loss结果来看，存在着非常大的震荡，从而导致无法正确估计参数a和b的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 正确版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, requires_grad = True) #创建a变量，并随机赋值初始化\n",
    "b = torch.rand(1, requires_grad = True) #创建b变量，并随机赋值初始化\n",
    "print('Initial parameters:', [a, b])\n",
    "learning_rate = 0.0001 #设置学习率\n",
    "for i in range(1000):\n",
    "    predictions = a * x_train + b  #计算在当前a、b条件下的模型预测数值\n",
    "    loss = torch.mean((predictions - y_train) ** 2) #通过与标签数据y比较，计算误差\n",
    "    print('loss:', loss)\n",
    "    loss.backward() #对损失函数进行梯度反传\n",
    "    a.data.add_(- learning_rate * a.grad.data)  #利用上一步计算中得到的a的梯度信息更新a中的data数值\n",
    "    b.data.add_(- learning_rate * b.grad.data)  #利用上一步计算中得到的b的梯度信息更新b中的data数值\n",
    "    ### 增加了这部分代码，清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加\n",
    "    a.grad.data.zero_() #清空a的梯度数值\n",
    "    b.grad.data.zero_() #清空b的梯度数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_train.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y_train.numpy(), 'o') # 绘制原始数据\n",
    "yplot, = plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "str1 = str(a.data.numpy()[0]) + 'x +' + str(b.data.numpy()[0]) #图例信息\n",
    "plt.legend([xplot, yplot],['Data', str1]) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = a * x_test + b #计算模型的预测结果\n",
    "predictions #输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_train.data.numpy() # 获得x包裹的数据\n",
    "x_pred = x_test.data.numpy()\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "plt.plot(x_data, y_train.data.numpy(), 'o') # 绘制训练数据\n",
    "plt.plot(x_pred, y_test.data.numpy(), 's') # 绘制测试数据\n",
    "x_data = np.r_[x_data, x_test.data.numpy()]\n",
    "plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  #绘制拟合数据\n",
    "plt.plot(x_pred, a.data.numpy() * x_pred + b.data.numpy(), 'o') #绘制预测数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "str1 = str(a.data.numpy()[0]) + 'x +' + str(b.data.numpy()[0]) #图例信息\n",
    "plt.legend([xplot, yplot],['Data', str1]) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
