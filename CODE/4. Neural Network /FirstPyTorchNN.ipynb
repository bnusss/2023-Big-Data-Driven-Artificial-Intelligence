{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"摩拜\"需要我\n",
    "\n",
    "在这节课中，我们将设计人工神经网络对某地区租赁单车的使用情况进行预测，我们将这个问题分解为三个小问题：\n",
    "\n",
    "1. 输入节点为1个，隐含层为10个，输出节点数为1的小型人工神经网络，用数据的下标预测单车数量\n",
    "2. 输入节点为56个，隐含层为10个，输出节点数为1的人工神经网络，用数据库中的星期几、是否节假日、温度、湿度等属性预测单车数量\n",
    "3. 输入节点为56个，隐含层节点数为10个，输出节点数为2个的人工神经网络，用数据库中的星期几、是否节假日、温度、湿度等属性预测单车数量是大于平均值还是小于平均值\n",
    "\n",
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入需要使用的库\n",
    "import numpy as np\n",
    "import pandas as pd #读取csv文件的库\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 让输出的图形直接在Notebook中显示\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、准备工作：读入数据文件\n",
    "\n",
    "首先，我们读入数据，绘制图形，看看数据长成什么样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据到内存中，rides为一个dataframe对象\n",
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "rides = pd.read_csv(data_path)\n",
    "\n",
    "#看看数据长什么样子\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们取出最后一列的前50条记录来进行预测\n",
    "counts = rides['cnt'][:50]\n",
    "\n",
    "#获得变量x，它是1，2，……，50\n",
    "x = np.arange(len(counts))\n",
    "\n",
    "# 将counts转成预测变量（标签）：y\n",
    "y = np.array(counts)\n",
    "\n",
    "# 绘制一个图形，展示曲线长的样子\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "plt.plot(x, y, 'o-') # 绘制原始数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归尝试\n",
    "\n",
    "我们可以先尝试用线性回归来对曲线进行拟合，复习一下上节课学过的内容，尽管效果很差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们取出数据库的最后一列的前50条记录来进行预测\n",
    "counts = rides['cnt'][:50]\n",
    "\n",
    "# 创建变量x，它是1，2，……，50\n",
    "x = torch.tensor(np.arange(len(counts)), dtype=torch.double, requires_grad = True)\n",
    "\n",
    "# 将counts转成预测变量（标签）：y\n",
    "y = torch.tensor(np.array(counts), dtype=torch.double, requires_grad = True)\n",
    "\n",
    "a = torch.rand(1, dtype=torch.double, requires_grad = True) #创建a变量，并随机赋值初始化\n",
    "b = torch.rand(1, dtype=torch.double, requires_grad = True) #创建b变量，并随机赋值初始化\n",
    "print('Initial parameters:', [a, b])\n",
    "learning_rate = 0.00001 #设置学习率\n",
    "for i in range(10000):\n",
    "    ### 增加了这部分代码，清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加\n",
    "    predictions = a * x+ b  #计算在当前a、b条件下的模型预测数值\n",
    "    loss = torch.mean((predictions - y) ** 2) #通过与标签数据y比较，计算误差\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "      print('loss:', loss)\n",
    "    loss.backward() #对损失函数进行梯度反传\n",
    "    a.data.add_(- learning_rate * a.grad.data)  #利用上一步计算中得到的a的梯度信息更新a中的data数值\n",
    "    b.data.add_(- learning_rate * b.grad.data)  #利用上一步计算中得到的b的梯度信息更新b中的data数值\n",
    "    a.grad.data.zero_() #清空a的梯度数值\n",
    "    b.grad.data.zero_() #清空b的梯度数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图形，展现线性回归的效果，结果惨不忍睹\n",
    "\n",
    "x_data = x.data.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据\n",
    "\n",
    "yplot, = plt.plot(x_data, predictions.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "str1 = str(a.data.numpy()[0]) + 'x +' + str(b.data.numpy()[0]) #图例信息\n",
    "plt.legend([xplot, yplot],['Data', str1]) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、第一个人工神经网络预测器\n",
    "\n",
    "我们构建一个单一输入，10个隐含层单元，1个输出单元的人工神经网络预测器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 慢速版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取出数据库中的最后一列的前50条记录来进行预测\n",
    "counts = rides['cnt'][:50]\n",
    "\n",
    "#创建变量x，它是1，2，……，50\n",
    "x = torch.tensor(np.arange(len(counts), dtype = float), requires_grad = True)\n",
    "\n",
    "# 将counts转成预测变量（标签）：y\n",
    "y = torch.tensor(np.array(counts, dtype = float), requires_grad = True)\n",
    "\n",
    "# 设置隐含层神经元的数量\n",
    "sz = 10\n",
    "\n",
    "# 初始化所有神经网络的权重（weights）和阈值（biases）\n",
    "weights = torch.randn((1, sz), dtype = torch.double, requires_grad = True) #1*10的输入到隐含层的权重矩阵\n",
    "biases = torch.randn(sz, dtype = torch.double, requires_grad = True) #尺度为10的隐含层节点偏置向量\n",
    "weights2 = torch.randn((sz, 1), dtype = torch.double, requires_grad = True) #10*1的隐含到输出层权重矩阵\n",
    "\n",
    "learning_rate = 0.001 #设置学习率\n",
    "losses = []\n",
    "\n",
    "# 将 x 转换为(50,1)的维度，以便与维度为(1,10)的weights矩阵相乘\n",
    "x = x.view(50, -1)\n",
    "# 将 y 转换为(50,1)的维度\n",
    "y = y.view(50, -1)\n",
    "\n",
    "for i in range(100000):\n",
    "    # 从输入层到隐含层的计算\n",
    "    hidden = x * weights + biases\n",
    "    # 将sigmoid函数作用在隐含层的每一个神经元上\n",
    "    hidden = torch.sigmoid(hidden)\n",
    "    #print(hidden.size())\n",
    "    # 隐含层输出到输出层，计算得到最终预测\n",
    "    predictions = hidden.mm(weights2)#\n",
    "    #print(predictions.size())\n",
    "    # 通过与标签数据y比较，计算均方误差\n",
    "    loss = torch.mean((predictions - y) ** 2) \n",
    "    #print(loss.size())\n",
    "    losses.append(loss.data.numpy())\n",
    "    \n",
    "    # 每隔10000个周期打印一下损失函数数值\n",
    "    if i % 10000 == 0:\n",
    "        print('loss:', loss)\n",
    "        \n",
    "    #对损失函数进行梯度反传\n",
    "    loss.backward()\n",
    "    \n",
    "    #利用上一步计算中得到的weights，biases等梯度信息更新weights或biases中的data数值\n",
    "    weights.data.add_(- learning_rate * weights.grad.data)  \n",
    "    biases.data.add_(- learning_rate * biases.grad.data)\n",
    "    weights2.data.add_(- learning_rate * weights2.grad.data)\n",
    "    \n",
    "    # 清空所有变量的梯度值。\n",
    "    # 因为pytorch中backward一次梯度信息会自动累加到各个变量上，因此需要清空，否则下一次迭代会累加，造成很大的偏差\n",
    "    weights.grad.data.zero_()\n",
    "    biases.grad.data.zero_()\n",
    "    weights2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印误差曲线\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x.data.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据\n",
    "\n",
    "yplot, = plt.plot(x_data, predictions.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "plt.legend([xplot, yplot],['Data', 'Prediction under 1000000 epochs']) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 改进版本\n",
    "\n",
    "上面的程序之所以跑得很慢，是因为x的取值范围1～50。\n",
    "而由于所有权重和biases的取值范围被设定为-1,1的正态分布随机数，这样就导致\n",
    "我们输入给隐含层节点的数值范围为-50~50，\n",
    "要想将sigmoid函数的多个峰值调节到我们期望的位置需要耗费很多的计算时间\n",
    "\n",
    "我们的解决方案就是将输入变量的范围归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出最后一列的前50条记录来进行预测\n",
    "counts = rides['cnt'][:50]\n",
    "\n",
    "#创建归一化的变量x，它的取值是0.02,0.04,...,1\n",
    "x = torch.tensor(np.arange(len(counts), dtype = float) / len(counts), requires_grad = True)\n",
    "\n",
    "# 创建归一化的预测变量y，它的取值范围是0～1\n",
    "y = torch.tensor(np.array(counts, dtype = float), requires_grad = True)\n",
    "\n",
    "# 初始化所有神经网络的权重（weights）和阈值（biases）\n",
    "weights = torch.randn((1, sz), dtype = torch.double, requires_grad = True) #1*10的输入到隐含层的权重矩阵\n",
    "biases = torch.randn(sz, dtype = torch.double, requires_grad = True) #尺度为10的隐含层节点偏置向量\n",
    "weights2 = torch.randn((sz, 1), dtype = torch.double, requires_grad = True) #10*1的隐含到输出层权重矩阵\n",
    "\n",
    "learning_rate = 0.001 #设置学习率\n",
    "losses = []\n",
    "\n",
    "# 将 x 转换为(50,1)的维度，以便与维度为(1,10)的weights矩阵相乘\n",
    "x = x.view(50, -1)\n",
    "# 将 y 转换为(50,1)的维度\n",
    "y = y.view(50, -1)\n",
    "\n",
    "for i in range(100000):\n",
    "    # 从输入层到隐含层的计算\n",
    "    hidden = x * weights + biases\n",
    "    # 将sigmoid函数作用在隐含层的每一个神经元上\n",
    "    hidden = torch.sigmoid(hidden)\n",
    "    # 隐含层输出到输出层，计算得到最终预测\n",
    "    predictions = hidden.mm(weights2)# + biases2.expand_as(y)\n",
    "    # 通过与标签数据y比较，计算均方误差\n",
    "    loss = torch.mean((predictions - y) ** 2) \n",
    "    losses.append(loss.data.numpy())\n",
    "    \n",
    "    # 每隔10000个周期打印一下损失函数数值\n",
    "    if i % 10000 == 0:\n",
    "        print('loss:', loss)\n",
    "        \n",
    "    #对损失函数进行梯度反传\n",
    "    loss.backward()\n",
    "    \n",
    "    #利用上一步计算中得到的weights，biases等梯度信息更新weights或biases中的data数值\n",
    "    weights.data.add_(- learning_rate * weights.grad.data)  \n",
    "    biases.data.add_(- learning_rate * biases.grad.data)\n",
    "    weights2.data.add_(- learning_rate * weights2.grad.data)\n",
    "    \n",
    "    # 清空所有变量的梯度值。\n",
    "    # 因为pytorch中backward一次梯度信息会自动累加到各个变量上，因此需要清空，否则下一次迭代会累加，造成很大的偏差\n",
    "    weights.grad.data.zero_()\n",
    "    biases.grad.data.zero_()\n",
    "    weights2.grad.data.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x.data.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据\n",
    "yplot, = plt.plot(x_data, predictions.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "plt.legend([xplot, yplot],['Data', 'Prediction']) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 用训练好的神经网络做预测\n",
    "\n",
    "预测下50个节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_predict = rides['cnt'][50:100] #读取待预测的接下来的50个数据点\n",
    "\n",
    "#首先对接下来的50个数据点进行选取，注意x应该取51，52，……，100，然后再归一化\n",
    "x = torch.tensor((np.arange(50, 100, dtype = float) / len(counts))\n",
    "                 , requires_grad = True)\n",
    "#读取下50个点的y数值，不需要做归一化\n",
    "y = torch.tensor(np.array(counts_predict, dtype = float), requires_grad = True)\n",
    "\n",
    "x = x.view(50, -1)\n",
    "y = y.view(50, -1)\n",
    "\n",
    "# 从输入层到隐含层的计算\n",
    "hidden = x * weights + biases\n",
    "\n",
    "# 将sigmoid函数作用在隐含层的每一个神经元上\n",
    "hidden = torch.sigmoid(hidden)\n",
    "\n",
    "# 隐含层输出到输出层，计算得到最终预测\n",
    "predictions = hidden.mm(weights2)\n",
    "\n",
    "# 计算预测数据上的损失函数\n",
    "loss = torch.mean((predictions - y) ** 2) \n",
    "print(loss)\n",
    "\n",
    "\n",
    "x_data = x.data.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据\n",
    "yplot, = plt.plot(x_data, predictions.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "plt.legend([xplot, yplot],['Data', 'Prediction']) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，预测发现存在着非常严重的过拟合现象！原因是x和y根本就没有关系！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、人工神经网络Neu\n",
    "\n",
    "在这一小节中，我们将再构建一个人工神经网络，利用数据库中的星期几、节假日、时间、风速等信息预测共享单车的使用数量\n",
    "\n",
    "该神经网络有56个输入层节点、10个隐含层节点和1个输出节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据的预处理过程\n",
    "\n",
    "要读入其他的数据就要考虑到这些数据具有不同的数据类型，以及取值范围，所以要对它们进行预处理\n",
    "\n",
    "另外，由于我们利用了全部数据来训练神经网络，所以采用之前介绍的一次性在全部数据上训练网络的方法就会很慢，\n",
    "所以我们将数据划分成了不同的撮（batch），一个批次一个批次地训练神经网络，因此我们要对数据进行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先，让我们再来看看数据长什么样子\n",
    "#读取数据到内存中，rides为一个dataframe对象\n",
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "rides = pd.read_csv(data_path)\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 对于类型变量的处理\n",
    "\n",
    "有很多变量都属于类型变量，例如season=1,2,3,4，分四季。我们不能将season变量直接输入到神经网络，这是因为season数值越高并不表示相应的信号强度越大。我们的解决方案是将类型变量用一个“一位热码“（one-hot）来编码，也就是：\n",
    "\n",
    "$\n",
    "season = 1 \\rightarrow (1, 0, 0 ,0) \\\\\n",
    "season = 2 \\rightarrow (0, 1, 0, 0) \\\\\n",
    "season = 3 \\rightarrow (0, 0, 1, 0) \\\\\n",
    "season = 4 \\rightarrow (0, 0, 0, 1) \\\\\n",
    "$\n",
    "\n",
    "因此，如果一个类型变量有n个不同取值，那么我们的“一位热码“所对应的向量长度就为n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于类型变量的特殊处理\n",
    "# season=1,2,3,4, weathersi=1,2,3, mnth= 1,2,...,12, hr=0,1, ...,23, weekday=0,1,...,6\n",
    "# 经过下面的处理后，将会多出若干特征，例如，对于season变量就会有 season_1, season_2, season_3, season_4\n",
    "# 这四种不同的特征。\n",
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    #利用pandas对象，我们可以很方便地将一个类型变量属性进行one-hot编码，变成多个属性\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "# 把原有的类型变量对应的特征去掉，将一些不相关的特征去掉\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 对于数值类型变量进行标准化\n",
    "由于每个数值型变量都是相互独立的，所以它们的数值绝对大小与问题本身没有关系，为了消除数值大小的差异，我们对每一个数值型变量进行标准化处理，也就是让其数值都围绕着0左右波动。比如，对于温度temp这个变量来说，它在整个数据库取值的平均着为mean(temp), 方差为std(temp)，所以，归一化的温度计算为：\n",
    "\n",
    "$ temp'=\\frac{temp - mean(temp)}{std(temp)}$\n",
    "\n",
    "这样做的好处就是可以将不同的取值范围的变量设置为让它们处于一个平等的地位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整所有的特征，标准化处理\n",
    "quant_features = ['cnt', 'temp', 'hum', 'windspeed']\n",
    "#quant_features = ['temp', 'hum', 'windspeed']\n",
    "\n",
    "# 我们将每一个变量的均值和方差都存储到scaled_features变量中。\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['temp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides['temp'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. 将数据集进行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有的数据集分为测试集和训练集，我们以后21天数据一共21*24个数据点作为测试集，其它是训练集\n",
    "test_data = data[-21*24:]\n",
    "train_data = data[:-21*24]\n",
    "print('训练数据：',len(train_data),'测试数据：',len(test_data))\n",
    "\n",
    "# 将我们的数据列分为特征列和目标列\n",
    "\n",
    "#目标列\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = train_data.drop(target_fields, axis=1), train_data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]\n",
    "\n",
    "# 将数据从pandas dataframe转换为numpy\n",
    "X = features.values\n",
    "Y = targets['cnt'].values\n",
    "Y = Y.astype(float)\n",
    "\n",
    "Y = np.reshape(Y, [len(Y),1])\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构建神经网络并进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 手动编写用Tensor运算的人工神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层\n",
    "input_size = features.shape[1] #输入层单元个数\n",
    "hidden_size = 10 #隐含层单元个数\n",
    "output_size = 1 #输出层单元个数\n",
    "batch_size = 128 #每隔batch的记录数\n",
    "weights1 = torch.randn([input_size, hidden_size], dtype = torch.double,  requires_grad = True) #第一到二层权重\n",
    "biases1 = torch.randn([hidden_size], dtype = torch.double, requires_grad = True) #隐含层偏置\n",
    "weights2 = torch.randn([hidden_size, output_size], dtype = torch.double, requires_grad = True) #隐含层到输出层权重\n",
    "def neu(x):\n",
    "    #计算隐含层输出\n",
    "    #x为batch_size * input_size的矩阵，weights1为input_size*hidden_size矩阵，\n",
    "    #biases为hidden_size向量，输出为batch_size * hidden_size矩阵    \n",
    "    hidden = x.mm(weights1) + biases1.expand(x.size()[0], hidden_size)\n",
    "    hidden = torch.sigmoid(hidden)\n",
    "    \n",
    "    #输入batch_size * hidden_size矩阵，mm上weights2, hidden_size*output_size矩阵，\n",
    "    #输出batch_size*output_size矩阵\n",
    "    output = hidden.mm(weights2)\n",
    "    return output\n",
    "def cost(x, y):\n",
    "    # 计算损失函数\n",
    "    error = torch.mean((x - y)**2)\n",
    "    return error\n",
    "def zero_grad():\n",
    "    # 清空每个参数的梯度信息\n",
    "    if weights1.grad is not None and biases1.grad is not None and weights2.grad is not None:\n",
    "        weights1.grad.data.zero_()\n",
    "        weights2.grad.data.zero_()\n",
    "        biases1.grad.data.zero_()\n",
    "def optimizer_step(learning_rate):\n",
    "    # 梯度下降算法\n",
    "    weights1.data.add_(- learning_rate * weights1.grad.data)\n",
    "    weights2.data.add_(- learning_rate * weights2.grad.data)\n",
    "    biases1.data.add_(- learning_rate * biases1.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络训练循环\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取\n",
    "    batch_loss = []\n",
    "    # start和end分别是提取一个batch数据的起始和终止下标\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = start + batch_size if start + batch_size < len(X) else len(X)\n",
    "        xx = torch.tensor(X[start:end], dtype = torch.double, requires_grad = True)\n",
    "        yy = torch.tensor(Y[start:end], dtype = torch.double, requires_grad = True)\n",
    "        predict = neu(xx)\n",
    "        loss = cost(predict, yy)\n",
    "        zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_step(0.01)\n",
    "        batch_loss.append(loss.data.numpy())\n",
    "    \n",
    "    # 每隔100步输出一下损失值（loss）\n",
    "    if i % 100==0:\n",
    "        losses.append(np.mean(batch_loss))\n",
    "        print(i, np.mean(batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印输出损失值\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(np.arange(len(losses))*100,losses, 'o-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 调用PyTorch现成的函数，构建序列化的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层\n",
    "input_size = features.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "batch_size = 128\n",
    "neu = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_size),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    ")\n",
    "cost = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(neu.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络训练循环\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取\n",
    "    batch_loss = []\n",
    "    # start和end分别是提取一个batch数据的起始和终止下标\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = start + batch_size if start + batch_size < len(X) else len(X)\n",
    "        xx = torch.tensor(X[start:end], dtype = torch.float, requires_grad = True)\n",
    "        yy = torch.tensor(Y[start:end], dtype = torch.float, requires_grad = True)\n",
    "        predict = neu(xx)\n",
    "        loss = cost(predict, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.data.numpy())\n",
    "    \n",
    "    # 每隔100步输出一下损失值（loss）\n",
    "    if i % 100==0:\n",
    "        losses.append(np.mean(batch_loss))\n",
    "        print(i, np.mean(batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印输出损失值\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(np.arange(len(losses))*100,losses, 'o-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 测试神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用训练好的神经网络在测试集上进行预测\n",
    "targets = test_targets['cnt'] #读取测试集的cnt数值\n",
    "targets = targets.values.reshape([len(targets),1]) #将数据转换成合适的tensor形式\n",
    "targets = targets.astype(float) #保证数据为实数\n",
    "\n",
    "x = torch.tensor(test_features.values, dtype = torch.float, requires_grad = True)\n",
    "y = torch.tensor(targets, dtype = torch.float, requires_grad = True)\n",
    "\n",
    "print(x[:10])\n",
    "# 用神经网络进行预测\n",
    "predict = neu(x)\n",
    "predict = predict.data.numpy()\n",
    "\n",
    "print((predict * std + mean)[:10])\n",
    "\n",
    "\n",
    "# 将后21天的预测数据与真实数据画在一起并比较\n",
    "# 横坐标轴是不同的日期，纵坐标轴是预测或者真实数据的值\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "ax.plot(predict * std + mean, label='Prediction', linestyle = '--')\n",
    "ax.plot(targets * std + mean, label='Data', linestyle = '-')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Date-time')\n",
    "ax.set_ylabel('Counts')\n",
    "# 对横坐标轴进行标注\n",
    "dates = pd.to_datetime(rides.loc[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 诊断网络*\n",
    "\n",
    "在这一小节我们对网络出现的问题进行诊断，看看哪一些神经元导致了预测偏差(集智AI学园出品，欢迎访问http://campus.swarma.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选出三天预测不准的日期：Dec 22，23，24\n",
    "# 将这三天的数据聚集到一起，存入subset和subtargets中\n",
    "bool1 = rides['dteday'] == '2012-12-22'\n",
    "bool2 = rides['dteday'] == '2012-12-23'\n",
    "bool3 = rides['dteday'] == '2012-12-24'\n",
    "\n",
    "# 将三个布尔型数组求与\n",
    "bools = [any(tup) for tup in zip(bool1,bool2,bool3) ]\n",
    "# 将相应的变量取出来\n",
    "subset = test_features.loc[rides[bools].index]\n",
    "subtargets = test_targets.loc[rides[bools].index]\n",
    "subtargets = subtargets['cnt']\n",
    "subtargets = subtargets.values.reshape([len(subtargets),1])\n",
    "\n",
    "def feature(X, net):\n",
    "    # 定义了一个函数可以提取网络的权重信息，所有的网络参数信息全部存储在了neu的named_parameters集合中了\n",
    "    X = torch.tensor(X, dtype = torch.float, requires_grad = False)\n",
    "    dic = dict(net.named_parameters()) #提取出来这个集合\n",
    "    weights = dic['0.weight'] #可以按照层数.名称来索引集合中的相应参数值\n",
    "    biases = dic['0.bias'] #可以按照层数.名称来索引集合中的相应参数值\n",
    "    h = torch.sigmoid(X.mm(weights.t()) + biases.expand([len(X), len(biases)])) # 隐含层的计算过程\n",
    "    return h # 输出层的计算\n",
    "\n",
    "# 将这几天的数据输入到神经网络中，读取出隐含层神经元的激活数值，存入results中\n",
    "results = feature(subset.values, neu).data.numpy()\n",
    "# 这些数据对应的预测值（输出层）\n",
    "predict = neu(torch.tensor(subset.values, dtype = torch.float, requires_grad = True)).data.numpy()\n",
    "\n",
    "#将预测值还原成原始数据的数值范围\n",
    "mean, std = scaled_features['cnt']\n",
    "predict = predict * std + mean\n",
    "subtargets = subtargets * std + mean\n",
    "# 将所有的神经元激活水平画在同一张图上，蓝色的是模型预测的数值\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.plot(results[:,:],'.:',alpha = 0.3)\n",
    "ax.plot((predict - min(predict)) / (max(predict) - min(predict)),'bs-',label='Prediction')\n",
    "ax.plot((subtargets - min(predict)) / (max(predict) - min(predict)),'ro-',label='Real')\n",
    "ax.plot(results[:, 3],':*',alpha=1, label='Neuro 4')\n",
    "\n",
    "ax.set_xlim(right=len(predict))\n",
    "ax.legend()\n",
    "plt.ylabel('Normalized Values')\n",
    "\n",
    "dates = pd.to_datetime(rides.loc[subset.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到了与峰值响应的神经元，把它到输入层的权重输出出来\n",
    "dic = dict(neu.named_parameters())\n",
    "weights = dic['2.weight']\n",
    "plt.plot(weights.data.numpy()[0],'o-')\n",
    "plt.xlabel('Input Neurons')\n",
    "plt.ylabel('Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in neu.named_parameters():\n",
    "    print(para) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到了与峰值相应的神经元，把它到输入层的权重输出出来\n",
    "dic = dict(neu.named_parameters())\n",
    "weights = dic['0.weight'][7]\n",
    "plt.plot(weights.data.numpy(),'o-')\n",
    "plt.xlabel('Input Neurons')\n",
    "plt.ylabel('Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出所有的features中的数据列，找到对应的编号\n",
    "for (i, c) in zip(range(len(features.columns)), features.columns):\n",
    "    print(i,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示在不同日期，指定的第7个隐含层神经元细胞的激活值，以及输入层响应\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "ax.plot(results[:,6],label='neuron in hidden')\n",
    "ax.plot(subset.values[:,33],label='neuron in input at 8am')\n",
    "ax.plot(subset.values[:,42],label='neuron in input at 5pm')\n",
    "ax.set_xlim(right=len(predict))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.loc[subset.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集智学园出品，欢迎访问http://campus.swarma.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
