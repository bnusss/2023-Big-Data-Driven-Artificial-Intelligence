{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我卷卷卷\n",
    "\n",
    "在这节课中，我们学习了如何使用torchvision加载图像库，如何构建一个简单的卷积神经网络，并了解如何训练这个卷积神经网络\n",
    "之后，我们还学会了如何对训练好的卷积网络进行分析\n",
    "\n",
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包，请保证torchvision已经在你的环境中安装好.\n",
    "# 在Windows需要单独安装torchvision包，在命令行运行pip install torchvision即可\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先，我们需要学习PyTorch自带的数据加载器，包括dataset，sampler，以及data loader这三个对象组成的套件。\n",
    "2. 当数据集很小，格式比较规则的时候，数据加载三套件的优势并不明显。但是当数据格式比较特殊，以及数据规模很大（内存无法同时加载所有数据）\n",
    "的时候，特别是，我们需要用不同的处理器来加载数据的时候，三套件的威力就会显现出来了。它会将数据加载、分布的任务自动完成。\n",
    "3. 在使用的时候，我们用dataset来装载数据集，用sampler来采样数据集。而对数据集的迭代、循环则主要通过data_loader来完成。\n",
    "4. 创建一个data_loader就需要一个dataset和一个datasampler，它基本实现的就是利用sampler自动从dataset种采样\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义超参数 \n",
    "image_size = 28  #图像的总尺寸28*28\n",
    "num_classes = 10  #标签的种类数\n",
    "num_epochs = 20  #训练的总循环周期\n",
    "batch_size = 64  #一个撮（批次）的大小，64张图片\n",
    "\n",
    "# 加载MINIST数据，如果没有下载过，就会在当前路径下新建/data子目录，并把文件存放其中\n",
    "# MNIST数据是属于torchvision包自带的数据，所以可以直接调用。\n",
    "# 在调用自己的数据的时候，我们可以用torchvision.datasets.ImageFolder或者torch.utils.data.TensorDataset来加载\n",
    "train_dataset = dsets.MNIST(root='./data',  #文件存放路径\n",
    "                            train=True,   #提取训练集\n",
    "                            transform=transforms.ToTensor(),  #将图像转化为Tensor，在加载数据的时候，就可以对图像做预处理\n",
    "                            download=True) #当找不到文件的时候，自动下载\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# 训练数据集的加载器，自动将数据分割成batch，顺序随机打乱\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "'''我们希望将测试数据分成两部分，一部分作为校验数据，一部分作为测试数据。\n",
    "校验数据用于检测模型是否过拟合，并调整参数，测试数据检验整个模型的工作'''\n",
    "\n",
    "\n",
    "# 首先，我们定义下标数组indices，它相当于对所有test_dataset中数据的编码\n",
    "# 然后定义下标indices_val来表示校验集数据的那些下标，indices_test表示测试集的下标\n",
    "indices = range(len(test_dataset))\n",
    "indices_val = indices[:5000]\n",
    "indices_test = indices[5000:]\n",
    "\n",
    "# 根据这些下标，构造两个数据集的SubsetRandomSampler采样器，它会对下标进行采样\n",
    "sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)\n",
    "sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)\n",
    "\n",
    "# 根据两个采样器来定义加载器，注意将sampler_val和sampler_test分别赋值给了validation_loader和test_loader\n",
    "validation_loader = torch.utils.data.DataLoader(dataset =test_dataset,\n",
    "                                                batch_size = batch_size,\n",
    "                                                sampler = sampler_val\n",
    "                                               )\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          sampler = sampler_test\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随便从数据集中读入一张图片，并绘制出来\n",
    "idx = 1000\n",
    "\n",
    "#dataset支持下标索引，其中提取出来的每一个元素为features，target格式，即属性和标签。[0]表示索引features\n",
    "muteimg = train_dataset[idx][0].numpy()\n",
    "#由于一般的图像包含rgb三个通道，而MINST数据集的图像都是灰度的，只有一个通道。因此，我们忽略通道，把图像看作一个灰度矩阵。\n",
    "#用imshow画图，会将灰度矩阵自动展现为彩色，不同灰度对应不同颜色：从黄到紫\n",
    "\n",
    "plt.imshow(muteimg[0,...])\n",
    "print('标签是：',train_dataset[idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、基本的卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 构建网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：在这里，我们将主要调用PyTorch强大的nn.Module这个类来构建卷积神经网络。我们分成如下这几个步骤：\n",
    "\n",
    "1. 首先，我们构造ConvNet类，它是对类nn.Module的继承（即nn.Module是父类，ConvNet为子类。如果这些概念不熟悉，请参考面向对象编程）\n",
    "2. 然后，我们复写__init__，以及forward这两个函数。第一个为构造函数，每当类ConvNet被具体化一个实例的时候，就会调用，forward则是\n",
    "在运行神经网络正向的时候会被自动调用\n",
    "3. 自定义自己的方法\n",
    "\n",
    "另一需要理解的是，ConvNet其实也是一个大容器，它里面有Conv2d，MaxPool2d等组件\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义卷积神经网络：4和8为人为指定的两个卷积层的厚度（feature map的数量）\n",
    "depth = [4, 8]\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 该函数在创建一个ConvNet对象的时候，即调用如下语句：net=ConvNet()，就会被调用\n",
    "        # 首先调用父类相应的构造函数\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # 其次构造ConvNet需要用到的各个神经模块。\n",
    "        '''注意，定义组件并没有真正搭建这些组件，只是把基本建筑砖块先找好'''\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding = 2) #定义一个卷积层，输入通道为1，输出通道为4，窗口大小为5，padding为2\n",
    "        self.pool = nn.MaxPool2d(2, 2) #定义一个Pooling层，一个窗口为2*2的pooling运算\n",
    "        self.conv2 = nn.Conv2d(depth[0], depth[1], 5, padding = 2) #第二层卷积，输入通道为depth[0], \n",
    "                                                                   #输出通道为depth[1]，窗口为5，padding为2\n",
    "        self.fc1 = nn.Linear(image_size // 4 * image_size // 4 * depth[1] , 512) \n",
    "                                                            #一个线性连接层，输入尺寸为最后一层立方体的平铺，输出层512个节点\n",
    "        self.fc2 = nn.Linear(512, num_classes) #最后一层线性分类单元，输入为512，输出为要做分类的类别数\n",
    "\n",
    "    def forward(self, x):\n",
    "        #该函数完成神经网络真正的前向运算，我们会在这里把各个组件进行实际的拼装\n",
    "        #x的尺寸：(batch_size, image_channels, image_width, image_height)\n",
    "        x = F.relu(self.conv1(x))  #第一层卷积，激活函数用ReLu，为了防止过拟合\n",
    "        #x的尺寸：(batch_size, num_filters, image_width, image_height)\n",
    "        x = self.pool(x) #第二层pooling，将图片变小\n",
    "        #x的尺寸：(batch_size, depth[0], image_width/2, image_height/2)\n",
    "        x = F.relu(self.conv2(x)) #第三层又是卷积，窗口为5，输入输出通道分别为depth[0]=4, depth[1]=8\n",
    "        #x的尺寸：(batch_size, depth[1], image_width/2, image_height/2)\n",
    "        x = self.pool(x) #第四层pooling，将图片缩小到原大小的1/4\n",
    "        #x的尺寸：(batch_size, depth[1], image_width/4, image_height/4)\n",
    "        \n",
    "        # 将立体的特征图Tensor，压成一个一维的向量\n",
    "        # view这个函数可以将一个tensor按指定的方式重新排布。\n",
    "        # 下面这个命令就是要让x按照batch_size * (image_size//4)^2*depth[1]的方式来排布向量\n",
    "        x = x.view(-1, image_size // 4 * image_size // 4 * depth[1])\n",
    "        #x的尺寸：(batch_size, depth[1]*image_width/4*image_height/4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) #第五层为全链接，ReLu激活函数\n",
    "        #x的尺寸：(batch_size, 512)\n",
    "\n",
    "        x = F.dropout(x, training=self.training) #以默认为0.5的概率对这一层进行dropout操作，为了防止过拟合\n",
    "        x = self.fc2(x) #全链接\n",
    "        #x的尺寸：(batch_size, num_classes)\n",
    "        \n",
    "        x = F.log_softmax(x, dim = 0) #输出层为log_softmax，即概率对数值log(p(x))。采用log_softmax可以使得后面的交叉熵计算更快\n",
    "        return x\n",
    "    \n",
    "    def retrieve_features(self, x):\n",
    "        #该函数专门用于提取卷积神经网络的特征图的功能，返回feature_map1, feature_map2为前两层卷积层的特征图\n",
    "        feature_map1 = F.relu(self.conv1(x)) #完成第一层卷积\n",
    "        x = self.pool(feature_map1)  # 完成第一层pooling\n",
    "        feature_map2 = F.relu(self.conv2(x)) #第二层卷积，两层特征图都存储到了feature_map1, feature_map2中\n",
    "        return (feature_map1, feature_map2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 训练这个卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightness(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案\"\"\"\n",
    "    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量\n",
    "    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = ConvNet() #新建一个卷积神经网络的实例，此时ConvNet的__init__函数就会被自动调用\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #Loss函数的定义，交叉熵\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) #定义优化器，普通的随机梯度下降算法\n",
    "\n",
    "record = [] #记录准确率等数值的容器\n",
    "weights = [] #每若干步就记录一次卷积核\n",
    "\n",
    "#开始训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_rights = [] #记录训练数据集准确率的容器\n",
    "    \n",
    "    ''' 下面的enumerate是构造一个枚举器的作用。就是我们在对train_loader做循环迭代的时候，enumerate会自动吐出一个数字指示我们循环了几次\n",
    "     这个数字就被记录在了batch_idx之中，它就等于0，1，2，……\n",
    "     train_loader每迭代一次，就会吐出来一对数据data和target，分别对应着一个batch中的手写数字图，以及对应的标签。'''\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  #针对容器中的每一个批进行循环\n",
    "        data, target = data.clone().requires_grad_(True), target.clone().detach()  #data为一批图像，target为一批标签\n",
    "        net.train() # 给网络模型做标记，标志说模型正在训练集上训练，\n",
    "                    #这种区分主要是为了打开关闭net的training标志，从而决定是否运行dropout\n",
    "            \n",
    "        output = net(data) #神经网络完成一次前馈的计算过程，得到预测输出output\n",
    "        loss = criterion(output, target) #将output与标签target比较，计算误差\n",
    "        optimizer.zero_grad() #清空梯度\n",
    "        loss.backward() #反向传播\n",
    "        optimizer.step() #一步随机梯度下降算法\n",
    "        right = rightness(output, target) #计算准确率所需数值，返回数值为（正确样例数，总样本数）\n",
    "        train_rights.append(right) #将计算结果装到列表容器train_rights中\n",
    "\n",
    "    \n",
    "        if batch_idx % 100 == 0: #每间隔100个batch执行一次打印等操作\n",
    "            \n",
    "            net.eval() # 给网络模型做标记，标志说模型在训练集上训练\n",
    "            val_rights = [] #记录校验数据集准确率的容器\n",
    "            \n",
    "            '''开始在校验数据集上做循环，计算校验集上面的准确度'''\n",
    "            for (data, target) in validation_loader:\n",
    "                data, target = data.clone().requires_grad_(True), target.clone().detach()\n",
    "                output = net(data) #完成一次前馈计算过程，得到目前训练得到的模型net在校验数据集上的表现\n",
    "                right = rightness(output, target) #计算准确率所需数值，返回正确的数值为（正确样例数，总样本数）\n",
    "                val_rights.append(right)\n",
    "            \n",
    "            # 分别计算在目前已经计算过的测试数据集，以及全部校验集上模型的表现：分类准确率\n",
    "            #train_r为一个二元组，分别记录目前已经经历过的所有训练集中分类正确的数量和该集合中总的样本数，\n",
    "            #train_r[0]/train_r[1]就是训练集的分类准确度，同样，val_r[0]/val_r[1]就是校验集上的分类准确度\n",
    "            train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))\n",
    "            #val_r为一个二元组，分别记录校验集中分类正确的数量和该集合中总的样本数\n",
    "            val_r = (sum([tup[0] for tup in val_rights]), sum([tup[1] for tup in val_rights]))\n",
    "            #打印准确率等数值，其中正确率为本训练周期Epoch开始后到目前撮的正确率的平均值\n",
    "            print(val_r)\n",
    "            print('训练周期: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t训练正确率: {:.2f}%\\t校验正确率: {:.2f}%'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.data, \n",
    "                100. * train_r[0].numpy() / train_r[1], \n",
    "                100. * val_r[0].numpy() / val_r[1]))\n",
    "            \n",
    "            #将准确率和权重等数值加载到容器中，以方便后续处理\n",
    "            record.append((100 - 100. * train_r[0] / train_r[1], 100 - 100. * val_r[0] / val_r[1]))\n",
    "            \n",
    "            # weights记录了训练周期中所有卷积核的演化过程。net.conv1.weight就提取出了第一层卷积核的权重\n",
    "            # clone的意思就是将weight.data中的数据做一个拷贝放到列表中，否则当weight.data变化的时候，列表中的每一项数值也会联动\n",
    "            '''这里使用clone这个函数很重要'''\n",
    "            weights.append([net.conv1.weight.data.clone(), net.conv1.bias.data.clone(), \n",
    "                            net.conv2.weight.data.clone(), net.conv2.bias.data.clone()])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制训练过程的误差曲线，校验集和测试集上的错误率。\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.plot(record) #record记载了每一个打印周期记录的训练和校验数据集上的准确度\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Error rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 在测试集上进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在测试集上分批运行，并计算总的正确率\n",
    "net.eval() #标志模型当前为运行阶段\n",
    "vals = [] #记录准确率所用列表\n",
    "\n",
    "#对测试数据集进行循环\n",
    "for data, target in test_loader:\n",
    "    data, target = data.clone().detach().requires_grad_(True), target.clone().detach()\n",
    "    output = net(data) #将特征数据喂入网络，得到分类的输出\n",
    "    val = rightness(output, target) #获得正确样本数以及总样本数\n",
    "    vals.append(val) #记录结果\n",
    "\n",
    "#计算准确率\n",
    "rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))\n",
    "right_rate = 100. * rights[0].numpy() / rights[1]\n",
    "print(right_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随便从测试集中读入一张图片，并检验模型的分类结果，并绘制出来\n",
    "idx = 4000\n",
    "muteimg = test_dataset[idx][0].numpy()\n",
    "plt.imshow(muteimg[0,...])\n",
    "print('标签是：',test_dataset[idx][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、解剖卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将训练好的卷积神经网络net进行解剖。我们主要关注一下几个问题：\n",
    "1. 第一层卷积核训练得到了什么；\n",
    "2. 第一层卷积核是如何在训练的过程中随着时间的演化而发生变化\n",
    "3. 在输入特定图像的时候，第一层卷积核所对应的4个featuremap是什么样子\n",
    "4. 第二层卷积核都是什么东西？\n",
    "5. 对于给定输入图像，第二层卷积核所对应的那些featuremaps都是什么样？\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 第一层卷积核、演化与特征图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一层卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取第一层卷积层的卷积核\n",
    "plt.figure(figsize = (10, 7))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(net.conv1.weight.data.numpy()[i,0,...]) #提取第一层卷积核中的权重值，注意conv1是net的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 滤波器的演化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将记录在容器中的卷积核权重历史演化数据打印出来\n",
    "i = 0\n",
    "for tup in weights:\n",
    "    if i % 10 == 0 :\n",
    "        layer1 = tup[0]\n",
    "        fig = plt.figure(figsize = (10, 7))\n",
    "        for j in range(4):\n",
    "            plt.subplot(1, 4, j + 1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(layer1.numpy()[j,0,...])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘制第一层特征图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用net的retrieve_features方法可以抽取出喂入当前数据后吐出来的所有特征图（第一个卷积和第二个卷积层）\n",
    "\n",
    "#首先定义读入的图片\n",
    "\n",
    "#它是从test_dataset中提取第idx个批次的第0个图，其次unsqueeze的作用是在最前面添加一维，\n",
    "#目的是为了让这个input_x的tensor是四维的，这样才能输入给net。补充的那一维表示batch。\n",
    "input_x = test_dataset[idx][0].unsqueeze(0) \n",
    "feature_maps = net.retrieve_features(input_x) #feature_maps是有两个元素的列表，分别表示第一层和第二层卷积的所有特征图\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "#有四个特征图，循环把它们打印出来\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(feature_maps[0][0, i,...].data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 绘制第二层卷积核与特征图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制第二层的卷积核，每一列对应一个卷积核，一共8个卷积核\n",
    "plt.figure(figsize = (15, 10))\n",
    "for i in range(4):\n",
    "    for j in range(8):\n",
    "        plt.subplot(4, 8, i * 8 + j + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(net.conv2.weight.data.numpy()[j, i,...])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制第二层的特征图，一共八个\n",
    "plt.figure(figsize = (10, 7))\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(feature_maps[1][0, i,...].data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 卷积神经网络的鲁棒性试验\n",
    "\n",
    "该试验设计如下：我们随机挑选一张测试图像，把它往左平移w个单位，然后：\n",
    "1. 考察分类结果是否变化\n",
    "2. 考察两层卷积对应的featuremap们有何变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取中test_dataset中的第idx个批次的第0个图的第0个通道对应的图像，定义为a。\n",
    "a = test_dataset[idx][0][0]\n",
    "\n",
    "# 平移后的新图像将放到b中。根据a给b赋值。\n",
    "b = torch.zeros(a.size()) #全0的28*28的矩阵\n",
    "w = 3 #平移的长度为3个像素\n",
    "\n",
    "# 对于b中的任意像素i,j，它等于a中的i,j+w这个位置的像素\n",
    "for i in range(a.size()[0]):\n",
    "    for j in range(0, a.size()[1] - w):\n",
    "        b[i, j] = a[i, j + w]\n",
    "\n",
    "# 将b画出来\n",
    "muteimg = b.numpy()\n",
    "plt.axis('off')\n",
    "plt.imshow(muteimg)\n",
    "\n",
    "# 把b喂给神经网络，得到分类结果pred（prediction是预测的每一个类别的概率的对数值），并把结果打印出来\n",
    "prediction = net(b.unsqueeze(0).unsqueeze(0))\n",
    "pred = torch.max(prediction.data, 1)[1]\n",
    "print(pred)\n",
    "\n",
    "#提取b对应的featuremap结果\n",
    "feature_maps = net.retrieve_features(b.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(feature_maps[0][0, i,...].data.numpy())\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i + 1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(feature_maps[1][0, i,...].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    # filename: 文件存放的路径，num_images: 读入的图片个数\n",
    "    \"\"\"将图像解压缩展开，读入成一个4维的张量： [image index（图像的编码）, y（纵坐标）, x（横坐标）, channels（通道）].\n",
    "    我们将数组中的数值范围从原来的[0, 255]降低到了[-0.5, 0.5]范围内\n",
    "    \"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "        data = data.reshape(num_images, NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"将label的数据文件解压缩，并将label读成64位的整数\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "# 将数据解压缩并存储到数组中，60000张图片，60000个label，测试集中有10000张图片\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# 将一部分图片（VALIDATION_SIZE=5000张）定为校验数据集\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "num_epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随便从数据集中读入一张图片，并绘制出来\n",
    "idx = 100\n",
    "muteimg = train_data[idx, 0, :, :]\n",
    "plt.imshow(muteimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录：不用dataloader版本的卷积神经网络\n",
    "\n",
    "如果不喜欢用PyTorch自带的dataset，dataloader等加载数据，可以用下面的代码来完成卷积神经网络的计算\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "#定义一系列常数\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/' #图像数据如果没下载，可以从这个地址下载\n",
    "WORK_DIRECTORY = 'data' #存储的路径名\n",
    "IMAGE_SIZE = 28 #每张图片的大小尺寸\n",
    "NUM_CHANNELS = 1  #每张图片的通道数\n",
    "PIXEL_DEPTH = 255 #像素的深度0-255\n",
    "NUM_LABELS = 10 #手写数字，一共十种\n",
    "VALIDATION_SIZE = 5000 #校验数据集大小\n",
    "NUM_EPOCHS = 20 # 训练的循环周期\n",
    "BATCH_SIZE = 64 #batch的大小\n",
    "EVAL_FREQUENCY = 100 #每隔100个batch进行一次校验集计算\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载图像文件，如果文件已经存在，那么就不下载。\n",
    "def maybe_download(filename):\n",
    "    \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "    if not os.path.isdir(WORK_DIRECTORY):\n",
    "        os.mkdir(WORK_DIRECTORY)\n",
    "    filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        size = os.path.getsize(filepath)\n",
    "        print('Successfully downloaded', filename, size, 'bytes.')\n",
    "    return filepath\n",
    "# Get the data.\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    # filename: 文件存放的路径，num_images: 读入的图片个数\n",
    "    \"\"\"将图像解压缩展开，读入成一个4维的张量： [image index（图像的编码）, y（纵坐标）, x（横坐标）, channels（通道）].\n",
    "    我们将数组中的数值范围从原来的[0, 255]降低到了[-0.5, 0.5]范围内\n",
    "    \"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "        data = data.reshape(num_images, NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"将label的数据文件解压缩，并将label读成64位的整数\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "# 将数据解压缩并存储到数组中，60000张图片，60000个label，测试集中有10000张图片\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# 将一部分图片（VALIDATION_SIZE=5000张）定为校验数据集\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "num_epochs = NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义关键函数\n",
    "# 我们在训练中用了一个Tensorflow图，在评价的过程中，我们拷贝了这个图\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，labels是数据之中的正确答案\"\"\"\n",
    "    predictions = np.argmax(predictions, 1)\n",
    "    return 100.0 - (\n",
    "      100.0 *\n",
    "      np.sum( predictions == labels) /\n",
    "      predictions.shape[0])\n",
    "\n",
    "def eval_in_batches(data, net):\n",
    "    \"\"\"计算得到预测精度，运行对所有的数据集中的小撮数据进行.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.ndarray(shape=(size, NUM_LABELS), dtype=np.float32)\n",
    "    \n",
    "    #一个小撮一个小撮地进行预测的计算\n",
    "    for begin in range(0, size, BATCH_SIZE):\n",
    "        end = begin + BATCH_SIZE\n",
    "        if end <= size:\n",
    "            inputs = data[begin:end]\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "            inputs = inputs.clone().detach().requires_grad_(True)\n",
    "            outputs = net(inputs)\n",
    "            predictions[begin:end, :] = outputs.data.numpy()\n",
    "        else:\n",
    "            inputs = data[-BATCH_SIZE:]\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "            inputs = inputs.clone().detach().requires_grad_(True)\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            batch_predictions = outputs.data.numpy()\n",
    "            predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_rec = []\n",
    "\n",
    "#获得训练集尺寸\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "print('Initialized!')\n",
    "\n",
    "net = ConvNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 开始训练循环，一共进行int(num_epochs * train_size) // BATCH_SIZE次\n",
    "print(int(num_epochs * train_size) // BATCH_SIZE)\n",
    "for step in range(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "    # 计算当前应该访问训练集中的哪一部分数据\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    batch_data = torch.from_numpy(batch_data)\n",
    "    batch_labels = torch.from_numpy(batch_labels)\n",
    "    \n",
    "    inputs, labels = batch_data.clone().detach(), batch_labels.clone().detach()\n",
    "    \n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "        #每间隔EVAL_FREQUENCY就打印一次预测结果\n",
    "        \n",
    "        print('第 %d (epoch %.2f) 步' %\n",
    "              (step, float(step) * BATCH_SIZE / train_size))\n",
    "        #print('损失函数值: %.3f, 学习率: %.6f' % (l, lr))\n",
    "        predictions = outputs.data.numpy()\n",
    "        #print(predictions.shape)\n",
    "        err_train = error_rate(predictions, labels.data.numpy())\n",
    "        prediction = eval_in_batches(validation_data, net)\n",
    "        err_valid = error_rate(prediction, validation_labels)\n",
    "        print('训练测试率: %.1f%%' % err_train)\n",
    "        print('校验集测试率: %.1f%%' % err_valid)\n",
    "        error_rate_rec.append([err_train,err_valid])\n",
    "        sys.stdout.flush()\n",
    "# 在测试集上实验\n",
    "prediction = eval_in_batches(test_data, net)\n",
    "test_error = error_rate(prediction, test_labels)\n",
    "print('测试误差: %.1f%%' % test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_rate_rec)\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#将测试集喂进去，得到计算结果，看一看预测的准确度\n",
    "right = 0\n",
    "batch_num = len(test_data) // BATCH_SIZE\n",
    "for i in range(batch_num):\n",
    "    inputs = test_data[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    inputs = inputs.clone().detach()\n",
    "    result = net(inputs)\n",
    "    result = result.data.numpy()\n",
    "    \n",
    "    labels = test_labels[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]\n",
    "    right += np.sum(np.argmax(result, 1) == labels)\n",
    "print(right / float(batch_num * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
