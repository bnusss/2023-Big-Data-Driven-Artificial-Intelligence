{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器也懂情感：文本分类\n",
    "\n",
    "在本课中，我们通过抓取京东商场上面的评论形成我们的语料。然后，我们训练一个模型来对这些评论进行分类，分为正面的评论和负面的评论。\n",
    "\n",
    "首先，我们展示了如何写一个简单的爬虫程序从网上扒文章；\n",
    "\n",
    "其次，我们设计了一个简单的前馈网络，利用词袋模型获得每一个评论的向量表示，并输入进前馈网络得到很好的分类效果，我们还对这个网络进行了简单的剖析。\n",
    "\n",
    "然后，我们尝试了两种RNN网络，一种是普通的RNN，另一种是LSTM。本程序展示了它们在处理同样的文本分类问题上的用法\n",
    "\n",
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第VI课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入程序所需要的程序包\n",
    "\n",
    "#抓取网页内容用的程序包\n",
    "import json\n",
    "import requests\n",
    "\n",
    "#PyTorch用的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "# 自然语言处理相关的包\n",
    "import re #正则表达式的包\n",
    "import jieba #结巴分词包\n",
    "from collections import Counter #搜集器，可以让统计词频更简单\n",
    "\n",
    "#绘图、计算用的程序包\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的数据来源于京东上的商品评论，每一条评论都会配合有一个评分。我们通过调用接口将相应的参数传入进去就可以得到评论。\n",
    "\n",
    "根据评分的高低，我们可以划分成正向和负向两组标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、从京东上抓取评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 在指定的url处获得评论\n",
    "# def get_comments(url):\n",
    "#     comments = []\n",
    "#     # 打开指定页面\n",
    "#     resp = requests.get(url)\n",
    "#     resp.encoding = 'gbk'\n",
    "    \n",
    "#     #如果200秒没有打开则失败\n",
    "#     if resp.status_code != 200:\n",
    "#         return []\n",
    "    \n",
    "#     #获得内容\n",
    "#     content = resp.text\n",
    "#     if content:\n",
    "#         #获得（）括号中的内容\n",
    "#         ind = content.find('(')\n",
    "#         s1 = content[ind+1:-2]\n",
    "#         try:\n",
    "#             #尝试利用jason接口来读取内容，并做jason的解析\n",
    "#             js = json.loads(s1)\n",
    "#             #提取出comments字段的内容\n",
    "#             comment_infos = js['comments']\n",
    "#         except:\n",
    "#             print('error')\n",
    "#             return([])\n",
    "        \n",
    "#         #对每一条评论进行内容部分的抽取\n",
    "#         for comment_info in comment_infos:\n",
    "#             comment_content = comment_info['content']\n",
    "#             str1 = comment_content + '\\n'\n",
    "#             comments.append(str1)\n",
    "#     return comments\n",
    "\n",
    "# good_comments = []\n",
    "\n",
    "# #评论抓取的来源地址，其中参数包括：\n",
    "# #productId为商品的id，score为评分，page为对应的评论翻页的页码，pageSize为总页数\n",
    "# #这里，我们设定score＝3表示好的评分。\n",
    "# good_comment_url_templates = [\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv8914&productId=10359162198&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv73&productId=10968941641&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv4653&productId=10335204102&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv1&productId=1269194114&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2777&productId=1409704820&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv174&productId=10103790891&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv9447&productId=1708318938&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv111&productId=10849803616&score=3&sortType=5&page={}&pageSize=10&isShadowSku=0'\n",
    "# ]\n",
    "\n",
    "# # 对上述网址进行循环，并模拟翻页100次\n",
    "# j=0\n",
    "# for good_comment_url_template in good_comment_url_templates:\n",
    "#     for i in range(100):\n",
    "#         url = good_comment_url_template.format(i)\n",
    "#         good_comments += get_comments(url)\n",
    "#         print('第{}条纪录，总文本长度{}'.format(j, len(good_comments)))\n",
    "#         j += 1\n",
    "# #将结果存储到good.txt文件中\n",
    "# fw = open('data/good.txt', 'w')\n",
    "# fw.writelines(good_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 负向评论如法炮制\n",
    "# bad_comments = []\n",
    "# bad_comment_url_templates = [\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv8914&productId=10359162198&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv73&productId=10968941641&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'http://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv4653&productId=10335204102&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv1&productId=1269194114&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2777&productId=1409704820&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv174&productId=10103790891&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv9447&productId=1708318938&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0',\n",
    "#     'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv111&productId=10849803616&score=1&sortType=5&page={}&pageSize=10&isShadowSku=0'\n",
    "# ]\n",
    "\n",
    "# j = 0\n",
    "# for bad_comment_url_template in bad_comment_url_templates:\n",
    "#     for i in range(100):\n",
    "#         url = bad_comment_url_template.format(i)\n",
    "#         bad_comments += get_comments(url)\n",
    "#         print('第{}条纪录，总文本长度{}'.format(j, len(bad_comments)))\n",
    "#         j += 1\n",
    "\n",
    "# fw = open('data/bad.txt', 'w')\n",
    "# fw.writelines(bad_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据来源文件\n",
    "good_file = 'data/good.txt'\n",
    "bad_file  = 'data/bad.txt'\n",
    "\n",
    "# 将文本中的标点符号过滤掉\n",
    "def filter_punc(sentence):\n",
    "    sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+\", \"\", sentence)  \n",
    "    return(sentence)\n",
    "\n",
    "#扫描所有的文本，分词、建立词典，分出正向还是负向的评论，is_filter可以过滤是否筛选掉标点符号\n",
    "def Prepare_data(good_file, bad_file, is_filter = True):\n",
    "    all_words = [] #存储所有的单词\n",
    "    pos_sentences = [] #存储正向的评论\n",
    "    neg_sentences = [] #存储负向的评论\n",
    "    with open(good_file, 'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                #过滤标点符号\n",
    "                line = filter_punc(line)\n",
    "            #分词\n",
    "            words = jieba.lcut(line)\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                pos_sentences.append(words)\n",
    "    print('{0} 包含 {1} 行, {2} 个词.'.format(good_file, idx+1, len(all_words)))\n",
    "\n",
    "    count = len(all_words)\n",
    "    with open(bad_file, 'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line)\n",
    "            words = jieba.lcut(line)\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                neg_sentences.append(words)\n",
    "    print('{0} 包含 {1} 行, {2} 个词.'.format(bad_file, idx+1, len(all_words)-count))\n",
    "\n",
    "    #建立词典，diction的每一项为{w:[id, 单词出现次数]}\n",
    "    diction = {}\n",
    "    cnt = Counter(all_words)\n",
    "    for word, freq in cnt.items():\n",
    "        diction[word] = [len(diction), freq]\n",
    "    print('字典大小：{}'.format(len(diction)))\n",
    "    return(pos_sentences, neg_sentences, diction)\n",
    "\n",
    "#根据单词返还单词的编码\n",
    "def word2index(word, diction):\n",
    "    if word in diction:\n",
    "        value = diction[word][0]\n",
    "    else:\n",
    "        value = -1\n",
    "    return(value)\n",
    "\n",
    "#根据编码获得单词\n",
    "def index2word(index, diction):\n",
    "    for w,v in diction.items():\n",
    "        if v[0] == index:\n",
    "            return(w)\n",
    "    return(None)\n",
    "\n",
    "pos_sentences, neg_sentences, diction = Prepare_data(good_file, bad_file, True)\n",
    "st = sorted([(v[1], w) for w, v in diction.items()])\n",
    "st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、词袋模型\n",
    "\n",
    "词袋模型实际上是一种对文本进行向量化的手段，通过统计出词表上的每个单词出现频率，从而将一篇文章向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 训练数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入一个句子和相应的词典，得到这个句子的向量化表示\n",
    "# 向量的尺寸为词典中词汇的个数，i位置上面的数值为第i个单词在sentence中出现的频率\n",
    "def sentence2vec(sentence, dictionary):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for l in sentence:\n",
    "        vector[l] += 1\n",
    "    return(1.0 * vector / len(sentence))\n",
    "\n",
    "# 遍历所有句子，将每一个词映射成编码\n",
    "dataset = [] #数据集\n",
    "labels = [] #标签\n",
    "sentences = [] #原始句子，调试用\n",
    "# 处理正向评论\n",
    "for sentence in pos_sentences:\n",
    "    new_sentence = []\n",
    "    for l in sentence:\n",
    "        if l in diction:\n",
    "            new_sentence.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence, diction))\n",
    "    labels.append(0) #正标签为0\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# 处理负向评论\n",
    "for sentence in neg_sentences:\n",
    "    new_sentence = []\n",
    "    for l in sentence:\n",
    "        if l in diction:\n",
    "            new_sentence.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence, diction))\n",
    "    labels.append(1) #负标签为1\n",
    "    sentences.append(sentence)\n",
    "\n",
    "#打乱所有的数据顺序，形成数据集\n",
    "# indices为所有数据下标的一个全排列\n",
    "indices = np.random.permutation(len(dataset))\n",
    "\n",
    "#重新根据打乱的下标生成数据集dataset，标签集labels，以及对应的原始句子sentences\n",
    "dataset = [dataset[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "sentences = [sentences[i] for i in indices]\n",
    "\n",
    "#对整个数据集进行划分，分为：训练集、校准集和测试集，其中校准和测试集合的长度都是整个数据集的10分之一\n",
    "test_size = len(dataset) // 10\n",
    "train_data = dataset[2 * test_size :]\n",
    "train_label = labels[2 * test_size :]\n",
    "\n",
    "valid_data = dataset[: test_size]\n",
    "valid_label = labels[: test_size]\n",
    "\n",
    "test_data = dataset[test_size : 2 * test_size]\n",
    "test_label = labels[test_size : 2 * test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的前馈神经网络，三层，第一层线性层，加一个非线性ReLU，第二层线性层，中间有10个隐含层神经元\n",
    "\n",
    "# 输入维度为词典的大小：每一段评论的词袋模型\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(len(diction), 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "def rightness(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案\"\"\"\n",
    "    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量\n",
    "    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数为交叉熵\n",
    "cost = torch.nn.NLLLoss()\n",
    "# 优化算法为Adam，可以自动调节学习率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "records = []\n",
    "\n",
    "#循环10个Epoch\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(zip(train_data, train_label)):\n",
    "        x, y = data\n",
    "        \n",
    "        # 需要将输入的数据进行适当的变形，主要是要多出一个batch_size的维度，也即第一个为1的维度\n",
    "        x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)\n",
    "        # x的尺寸：batch_size=1, len_dictionary\n",
    "        # 标签也要加一层外衣以变成1*1的张量\n",
    "        y = torch.tensor(np.array([y]), dtype = torch.long)\n",
    "        # y的尺寸：batch_size=1, 1\n",
    "        \n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 模型预测\n",
    "        predict = model(x)\n",
    "        # 计算损失函数\n",
    "        loss = cost(predict, y)\n",
    "        # 将损失函数数值加入到列表中\n",
    "        losses.append(loss.data.numpy())\n",
    "        # 开始进行梯度反传\n",
    "        loss.backward()\n",
    "        # 开始对参数进行一步优化\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 每隔3000步，跑一下校验数据集的数据，输出临时结果\n",
    "        if i % 3000 == 0:\n",
    "            val_losses = []\n",
    "            rights = []\n",
    "            # 在所有校验数据集上实验\n",
    "            for j, val in enumerate(zip(valid_data, valid_label)):\n",
    "                x, y = val\n",
    "                x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)\n",
    "                y = torch.tensor(np.array([y]), dtype = torch.long)\n",
    "                predict = model(x)\n",
    "                # 调用rightness函数计算准确度\n",
    "                right = rightness(predict, y)\n",
    "                rights.append(right)\n",
    "                loss = cost(predict, y)\n",
    "                val_losses.append(loss.data.numpy())\n",
    "                \n",
    "            # 将校验集合上面的平均准确度计算出来\n",
    "            right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "            print('第{}轮，训练损失：{:.2f}, 校验损失：{:.2f}, 校验准确率: {:.2f}'.format(epoch, np.mean(losses),\n",
    "                                                                        np.mean(val_losses), right_ratio))\n",
    "            records.append([np.mean(losses), np.mean(val_losses), right_ratio])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制误差曲线\n",
    "a = [i[0] for i in records]\n",
    "b = [i[1] for i in records]\n",
    "c = [i[2] for i in records]\n",
    "plt.plot(a, label = 'Train Loss')\n",
    "plt.plot(b, label = 'Valid Loss')\n",
    "plt.plot(c, label = 'Valid Accuracy')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存、提取模型（为展示用）\n",
    "#torch.save(model,'bow.mdl')\n",
    "#model = torch.load('bow.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在测试集上分批运行，并计算总的正确率\n",
    "vals = [] #记录准确率所用列表\n",
    "\n",
    "#对测试数据集进行循环\n",
    "for data, target in zip(test_data, test_label):\n",
    "    data, target = torch.tensor(data, dtype = torch.float).view(1,-1), torch.tensor(np.array([target]), dtype = torch.long)\n",
    "    output = model(data) #将特征数据喂入网络，得到分类的输出\n",
    "    val = rightness(output, target) #获得正确样本数以及总样本数\n",
    "    vals.append(val) #记录结果\n",
    "\n",
    "#计算准确率\n",
    "rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))\n",
    "right_rate = 1.0 * rights[0].data.numpy() / rights[1]\n",
    "right_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. 解剖神经网络\n",
    "\n",
    "接下来，我们对训练好的神经网络进行解剖分析。\n",
    "\n",
    "我们看一看每一个神经元都在检测什么模式；\n",
    "\n",
    "我们也希望看到神经网络在测试集上判断错误的数据上出错的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1). 查看每一层的模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将神经网络的架构打印出来，方便后面的访问\n",
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制出第二个全链接层的权重大小\n",
    "# model[2]即提取第2层，网络一共4层，第0层为线性神经元，第1层为ReLU，第2层为第二层神经原链接，第3层为logsoftmax\n",
    "plt.figure(figsize = (10, 7))\n",
    "for i in range(model[2].weight.size()[0]):\n",
    "    #if i == 1:\n",
    "        weights = model[2].weight[i].data.numpy()\n",
    "        plt.plot(weights, 'o-', label = i)\n",
    "plt.legend()\n",
    "plt.xlabel('Neuron in Hidden Layer')\n",
    "plt.ylabel('Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将第一层神经元的权重都打印出来，一条曲线表示一个隐含层神经元。横坐标为输入层神经元编号，纵坐标为权重值大小\n",
    "plt.figure(figsize = (10, 7))\n",
    "for i in range(model[0].weight.size()[0]):\n",
    "    #if i == 1:\n",
    "        weights = model[0].weight[i].data.numpy()\n",
    "        plt.plot(weights, alpha = 0.5, label = i)\n",
    "plt.legend()\n",
    "plt.xlabel('Neuron in Input Layer')\n",
    "plt.ylabel('Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将第二层的各个神经元与输入层的链接权重，挑出来最大的权重和最小的权重，并考察每一个权重所对应的单词是什么，把单词打印出来\n",
    "# model[0]是取出第一层的神经元\n",
    "\n",
    "for i in range(len(model[0].weight)):\n",
    "    print('\\n')\n",
    "    print('第{}个神经元'.format(i))\n",
    "    print('max:')\n",
    "    st = sorted([(w,i) for i,w in enumerate(model[0].weight[i].data.numpy())])\n",
    "    for i in range(1, 20):\n",
    "        word = index2word(st[-i][1],diction)\n",
    "        print(word)\n",
    "    print('min:')\n",
    "    for i in range(20):\n",
    "        word = index2word(st[i][1],diction)\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 寻找判断错误的原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集到在测试集中判断错误的句子\n",
    "wrong_sentences = []\n",
    "targets = []\n",
    "j = 0\n",
    "sent_indices = []\n",
    "for data, target in zip(test_data, test_label):\n",
    "    predictions = model(torch.tensor(data, dtype = torch.float).view(1,-1))\n",
    "    pred = torch.max(predictions.data, 1)[1]\n",
    "    target = torch.tensor(np.array([target]), dtype = torch.long).view_as(pred)\n",
    "    rights = pred.eq(target)\n",
    "    indices = np.where(rights.numpy() == 0)[0]\n",
    "    for i in indices:\n",
    "        wrong_sentences.append(data)\n",
    "        targets.append(target[i])\n",
    "        sent_indices.append(test_size + j + i)\n",
    "    j += len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐个查看出错的句子是什么\n",
    "idx = 1\n",
    "print(sent_indices)\n",
    "print(sentences[sent_indices[idx]])\n",
    "print(targets[idx].numpy())\n",
    "lst = list(np.where(wrong_sentences[idx]>0)[0])\n",
    "mm = list(map(lambda x:index2word(x, diction), lst))\n",
    "print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察第一层的权重与输入向量的内积结果，也就是对隐含层神经元的输入，其中最大数值对应的项就是被激活的神经元\n",
    "# 负值最小的神经元就是被抑制的神经元\n",
    "model[0].weight.data.numpy().dot(wrong_sentences[idx].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示输入句子的非零项，即对应单词不为空的项，看它们到隐含层指定神经元的权重是多少\n",
    "model[0].weight[0].data.numpy()[np.where(wrong_sentences[idx]>0)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、RNN模型\n",
    "\n",
    "我们分别比较了两种RNN模型，一个是普通的RNN模型，另一个是LSTM。\n",
    "\n",
    "本单元的主要目的是了解RNN模型如何实现，以及考察它们在测试数据集上的分类准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 普通RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要重新数据预处理，主要是要加上标点符号，它对于RNN起到重要作用\n",
    "# 数据来源文件\n",
    "good_file = 'data/good.txt'\n",
    "bad_file  = 'data/bad.txt'\n",
    "# 生成正样例和反样例，以及词典，很有趣的是，词典中的词语竟然比不考虑标点符号的时候少了（要知道标点也是被当作一个单词的），\n",
    "# 主要原因应该是总的分词出来的数量变少了。当去掉标点符号以后，有很多字的组合被当作了单词处理了。\n",
    "pos_sentences, neg_sentences, diction = Prepare_data(good_file, bad_file, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新准备数据，输入给RNN\n",
    "# 与词袋模型不同的是。每一个句子在词袋模型中都被表示为了固定长度的向量，其中长度为字典的尺寸\n",
    "# 在RNN中，每一个句子就是被单独当成词语的序列来处理的，因此序列的长度是与句子等长的\n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "sentences = []\n",
    "\n",
    "# 正例集合\n",
    "for sentence in pos_sentences:\n",
    "    new_sentence = []\n",
    "    for l in sentence:\n",
    "        if l in diction:\n",
    "            # 注意将每个词编码\n",
    "            new_sentence.append(word2index(l, diction))\n",
    "    #每一个句子都是一个不等长的整数序列\n",
    "    dataset.append(new_sentence)\n",
    "    labels.append(0)\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# 反例集合\n",
    "for sentence in neg_sentences:\n",
    "    new_sentence = []\n",
    "    for l in sentence:\n",
    "        if l in diction:\n",
    "            new_sentence.append(word2index(l, diction))\n",
    "    dataset.append(new_sentence)\n",
    "    labels.append(1)\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# 重新对数据洗牌，构造数据集合\n",
    "indices = np.random.permutation(len(dataset))\n",
    "dataset = [dataset[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "sentences = [sentences[i] for i in indices]\n",
    "\n",
    "test_size = len(dataset) // 10\n",
    "\n",
    "# 训练集\n",
    "train_data = dataset[2 * test_size :]\n",
    "train_label = labels[2 * test_size :]\n",
    "\n",
    "# 校验集\n",
    "valid_data = dataset[: test_size]\n",
    "valid_label = labels[: test_size]\n",
    "\n",
    "# 测试集\n",
    "test_data = dataset[test_size : 2 * test_size]\n",
    "test_label = labels[test_size : 2 * test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个手动实现的RNN模型\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # 一个embedding层\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        # 隐含层内部的相互链接\n",
    "        self.i2h = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        # 隐含层到输出层的链接\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        # 先进行embedding层的计算，它可以把一个数或者数列，映射成一个向量或一组向量\n",
    "        # input尺寸：seq_length, 1\n",
    "        x = self.embed(input)\n",
    "        # x尺寸：hidden_size\n",
    "        \n",
    "        # 将输入和隐含层的输出（hidden）耦合在一起构成了后续的输入\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        # combined尺寸：2*hidden_size\n",
    "        #\n",
    "        # 从输入到隐含层的计算\n",
    "        hidden = self.i2h(combined)\n",
    "        # combined尺寸：hidden_size\n",
    "        \n",
    "        # 从隐含层到输出层的运算\n",
    "        output = self.i2o(hidden)\n",
    "        # output尺寸：output_size\n",
    "        \n",
    "        # softmax函数\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # 对隐含单元的初始化\n",
    "        # 注意尺寸是：batch_size, hidden_size\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练这个RNN，10个隐含层单元\n",
    "rnn = RNN(len(diction), 10, 2)\n",
    "\n",
    "# 交叉熵评价函数\n",
    "cost = torch.nn.NLLLoss()\n",
    "\n",
    "# Adam优化器\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = 0.001)\n",
    "records = []\n",
    "\n",
    "# 学习周期10次\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i, data in enumerate(zip(train_data, train_label)):\n",
    "        x, y = data\n",
    "        x = torch.tensor(x, dtype = torch.long).unsqueeze(1)\n",
    "        #x尺寸：seq_length（序列的长度）\n",
    "        y = torch.tensor(np.array([y]), dtype = torch.long)\n",
    "        #x尺寸：batch_size = 1,1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #初始化隐含层单元全为0\n",
    "        hidden = rnn.initHidden()\n",
    "        # hidden尺寸：batch_size = 1, hidden_size\n",
    "        \n",
    "        #手动实现RNN的时间步循环，x的长度就是总的循环时间步，因为要把x中的输入句子全部读取完毕\n",
    "        for s in range(x.size()[0]):\n",
    "            output, hidden = rnn(x[s], hidden)\n",
    "        \n",
    "        #校验函数\n",
    "        loss = cost(output, y)\n",
    "        losses.append(loss.data.numpy())\n",
    "        loss.backward()\n",
    "        # 开始优化\n",
    "        optimizer.step()\n",
    "        if i % 3000 == 0:\n",
    "            # 每间隔3000步来一次校验集上面的计算\n",
    "            val_losses = []\n",
    "            rights = []\n",
    "            for j, val in enumerate(zip(valid_data, valid_label)):\n",
    "                x, y = val\n",
    "                x = torch.tensor(x, dtype = torch.long).unsqueeze(1)\n",
    "                y = torch.tensor(np.array([y]), dtype = torch.long)\n",
    "                hidden = rnn.initHidden()\n",
    "                for s in range(x.size()[0]):\n",
    "                    output, hidden = rnn(x[s], hidden)\n",
    "                right = rightness(output, y)\n",
    "                rights.append(right)\n",
    "                loss = cost(output, y)\n",
    "                val_losses.append(loss.data.numpy())\n",
    "            # 计算准确度\n",
    "            right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "            print('第{}轮，训练损失：{:.2f}, 测试损失：{:.2f}, 测试准确率: {:.2f}'.format(epoch, np.mean(losses),\n",
    "                                                                        np.mean(val_losses), right_ratio))\n",
    "            records.append([np.mean(losses), np.mean(val_losses), right_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制误差曲线\n",
    "a = [i[0] for i in records]\n",
    "b = [i[1] for i in records]\n",
    "c = [i[2] for i in records]\n",
    "plt.plot(a, label = 'Train Loss')\n",
    "plt.plot(b, label = 'Valid Loss')\n",
    "plt.plot(c, label = 'Valid Accuracy')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在测试集上运行，并计算准确率\n",
    "vals = [] #记录准确率所用列表\n",
    "rights = []\n",
    "#对测试数据集进行循环\n",
    "for j, test in enumerate(zip(test_data, test_label)):\n",
    "    x, y = test\n",
    "    x = torch.LongTensor(x).unsqueeze(1)\n",
    "    y = torch.LongTensor(np.array([y]))\n",
    "    hidden = rnn.initHidden()\n",
    "    for s in range(x.size()[0]):\n",
    "        output, hidden = rnn(x[s], hidden)\n",
    "    right = rightness(output, y)\n",
    "    rights.append(right)\n",
    "    val = rightness(output, y) #获得正确样本数以及总样本数\n",
    "    vals.append(val) #记录结果\n",
    "\n",
    "#计算准确率\n",
    "rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))\n",
    "right_rate = 1.0 * rights[0].data.numpy() / rights[1]\n",
    "right_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存、加载模型（为讲解用）\n",
    "#torch.save(rnn, 'rnn.mdl')\n",
    "#rnn = torch.load('rnn.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "普通RNN的效果并不好，我们尝试利用改进型的RNN，即LSTM。LSTM与RNN最大的区别就是在于每个神经元中多增加了3个控制门：遗忘门、输入门和输出门. 另外，在每个隐含层神经元中，LSTM多了一个cell的状态，起到了记忆的作用。\n",
    "\n",
    "这就使得LSTM可以记忆更长时间的Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM的构造如下：一个embedding层，将输入的任意一个单词映射为一个向量\n",
    "        # 一个LSTM隐含层，共有hidden_size个LSTM神经元\n",
    "        # 一个全链接层，外接一个softmax输出\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \n",
    "        #input尺寸: seq_length\n",
    "        #词向量嵌入\n",
    "        embedded = self.embedding(input)\n",
    "        #embedded尺寸: seq_length, hidden_size\n",
    "        \n",
    "        #PyTorch设计的LSTM层有一个特别别扭的地方是，输入张量的第一个维度需要是时间步，\n",
    "        #第二个维度才是batch_size，所以需要对embedded变形\n",
    "        embedded = embedded.view(input.data.size()[0], 1, self.hidden_size)\n",
    "        #embedded尺寸: seq_length, batch_size = 1, hidden_size\n",
    "    \n",
    "        #调用PyTorch自带的LSTM层函数，注意有两个输入，一个是输入层的输入，另一个是隐含层自身的输入\n",
    "        # 输出output是所有步的隐含神经元的输出结果，hidden是隐含层在最后一个时间步的状态。\n",
    "        # 注意hidden是一个tuple，包含了最后时间步的隐含层神经元的输出，以及每一个隐含层神经元的cell的状态\n",
    "        \n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        #output尺寸: seq_length, batch_size = 1, hidden_size\n",
    "        #hidden尺寸: 二元组(n_layer = 1 * batch_size = 1 * hidden_size, n_layer = 1 * batch_size = 1 * hidden_size)\n",
    "        \n",
    "        #我们要把最后一个时间步的隐含神经元输出结果拿出来，送给全连接层\n",
    "        output = output[-1,...]\n",
    "        #output尺寸: batch_size = 1, hidden_size\n",
    "\n",
    "        #全链接层\n",
    "        out = self.fc(output)\n",
    "        #out尺寸: batch_size = 1, output_size\n",
    "        # softmax\n",
    "        out = self.logsoftmax(out)\n",
    "        return out\n",
    "\n",
    "    def initHidden(self):\n",
    "        # 对隐单元的初始化\n",
    "        \n",
    "        # 对隐单元输出的初始化，全0.\n",
    "        # 注意hidden和cell的维度都是layers,batch_size,hidden_size\n",
    "        hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "        # 对隐单元内部的状态cell的初始化，全0\n",
    "        cell = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "        return (hidden, cell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练LSTM网络\n",
    "\n",
    "# 构造一个LSTM网络的实例\n",
    "lstm = LSTMNetwork(len(diction), 10, 2)\n",
    "\n",
    "#定义损失函数\n",
    "cost = torch.nn.NLLLoss()\n",
    "\n",
    "#定义优化器\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr = 0.001)\n",
    "records = []\n",
    "\n",
    "# 开始训练，一共5个epoch，否则容易过拟合\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(zip(train_data, train_label)):\n",
    "        x, y = data\n",
    "        x = torch.LongTensor(x).unsqueeze(1)\n",
    "        #x尺寸：seq_length，序列的长度\n",
    "        y = torch.LongTensor([y])\n",
    "        #y尺寸：batch_size = 1, 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #初始化LSTM隐含层单元的状态\n",
    "        hidden = lstm.initHidden()\n",
    "        #hidden: 二元组(n_layer = 1 * batch_size = 1 * hidden_size, n_layer = 1 * batch_size = 1 * hidden_size)\n",
    "        \n",
    "        #让LSTM开始做运算，注意，不需要手工编写对时间步的循环，而是直接交给PyTorch的LSTM层。\n",
    "        #它自动会根据数据的维度计算若干时间步\n",
    "        output = lstm(x, hidden)\n",
    "        #output尺寸: batch_size = 1, output_size\n",
    "        \n",
    "        #损失函数\n",
    "        loss = cost(output, y)\n",
    "        losses.append(loss.data.numpy())\n",
    "        \n",
    "        #反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #每隔3000步，跑一次校验集，并打印结果\n",
    "        if i % 3000 == 0:\n",
    "            val_losses = []\n",
    "            rights = []\n",
    "            for j, val in enumerate(zip(valid_data, valid_label)):\n",
    "                x, y = val\n",
    "                x = torch.LongTensor(x).unsqueeze(1)\n",
    "                y = torch.LongTensor(np.array([y]))\n",
    "                hidden = lstm.initHidden()\n",
    "                output = lstm(x, hidden)\n",
    "                #计算校验数据集上的分类准确度\n",
    "                right = rightness(output, y)\n",
    "                rights.append(right)\n",
    "                loss = cost(output, y)\n",
    "                val_losses.append(loss.data.numpy())\n",
    "            right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "            print('第{}轮，训练损失：{:.2f}, 测试损失：{:.2f}, 测试准确率: {:.2f}'.format(epoch, np.mean(losses),\n",
    "                                                                        np.mean(val_losses), right_ratio))\n",
    "            records.append([np.mean(losses), np.mean(val_losses), right_ratio])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制误差曲线\n",
    "a = [i[0] for i in records]\n",
    "b = [i[1] for i in records]\n",
    "c = [i[2] for i in records]\n",
    "plt.plot(a, label = 'Train Loss')\n",
    "plt.plot(b, label = 'Valid Loss')\n",
    "plt.plot(c, label = 'Valid Accuracy')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在测试集上计算总的正确率\n",
    "vals = [] #记录准确率所用列表\n",
    "\n",
    "#对测试数据集进行循环\n",
    "for j, test in enumerate(zip(test_data, test_label)):\n",
    "    x, y = test\n",
    "    x = torch.LongTensor(x).unsqueeze(1)\n",
    "    y = torch.LongTensor(np.array([y]))\n",
    "    hidden = lstm.initHidden()\n",
    "    output = lstm(x, hidden)\n",
    "    right = rightness(output, y)\n",
    "    rights.append(right)\n",
    "    val = rightness(output, y) #获得正确样本数以及总样本数\n",
    "    vals.append(val) #记录结果\n",
    "\n",
    "#计算准确率\n",
    "rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))\n",
    "right_rate = 1.0 * rights[0].data.numpy() / rights[1]\n",
    "right_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存、加载模型（为讲解用）\n",
    "#torch.save(lstm, 'lstm.mdl')\n",
    "#rnn = torch.load('rnn.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
