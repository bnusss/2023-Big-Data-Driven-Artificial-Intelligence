{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import entropy_estimators as ee\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import models\n",
    "from models import Renorm_Dynamic\n",
    "from EI_calculation import approx_ei\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "device =torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov = torch.Tensor([[1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0], [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0], \n",
    "                       [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0], [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0],\n",
    "                      [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0], [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0],\n",
    "                      [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 0], [0,0,0,0,0,0,0,1]])\n",
    "#markov = torch.Tensor([[1/3, 1/3, 1/3, 0], [1/3, 1/3, 1/3, 0], [1/3, 1/3, 1/3, 0],[0,0,0,1]])\n",
    "def one_step(state):\n",
    "    m = distributions.Categorical(state@markov)\n",
    "    s_next = torch.nn.functional.one_hot(m.sample())*1.0\n",
    "    return s_next\n",
    "def generate_data(batch_size):\n",
    "    distrs = distributions.Categorical(torch.ones(markov.size()[0])/markov.size()[0])\n",
    "    state = distrs.sample([batch_size, 1]).squeeze(1)\n",
    "    state_onehot = torch.nn.functional.one_hot(state, markov.size()[0])*1.0\n",
    "    state_next = one_step(state_onehot)\n",
    "    return state_onehot.to(device), state_next.to(device)\n",
    "def test_model(batch_size,input_range,net,scale,nll):\n",
    "    batch_size1 = batch_size*10\n",
    "    state, state_next = generate_data(batch_size)\n",
    "    predict, latent, latent_p = net(state) \n",
    "    predict = nn.functional.log_softmax(predict, dim=1)\n",
    "    loss = nll(predict, torch.nonzero(state_next)[:,1])\n",
    "    ssp = net.encoding(state_next)\n",
    "    sigmas = torch.sqrt(torch.mean((ssp-latent_p)**2, 0))\n",
    "    inv_sigma = torch.inverse(torch.diag(sigmas))\n",
    "    ei = approx_ei(scale, scale, inv_sigma.data, lambda x:(net.dynamics(x.unsqueeze(0))+x.unsqueeze(0)), 1000, 100)\n",
    "    one_ei = onehot_ei(input_range, lambda x: net(x))\n",
    "    return ei, sigmas,loss.item(), one_ei\n",
    "def onehot_ei(input_size, netfunc):\n",
    "    values = torch.LongTensor(range(input_size))\n",
    "    onehots = torch.nn.functional.one_hot(values)*1.0\n",
    "    pyx,_,_ = netfunc(onehots)\n",
    "    pyx = torch.softmax(pyx, dim=1)\n",
    "    logpyx=torch.log(pyx)\n",
    "    logpyx = torch.where(torch.isinf(logpyx), torch.zeros(onehots.size()), logpyx)\n",
    "    entropy = pyx * logpyx\n",
    "    sumz = torch.sum(pyx, 0).unsqueeze(0)\n",
    "    logsumz = torch.log(sumz)\n",
    "    logsumz = torch.where(torch.isinf(logsumz), torch.zeros(logsumz.size()), logsumz)\n",
    "    logsumz = logsumz.repeat(input_size, 1)\n",
    "    \n",
    "    #print(logsumz)\n",
    "    first_term = torch.sum(entropy)\n",
    "    second_term = torch.sum(pyx * logsumz)\n",
    "    \n",
    "    final = (first_term - second_term)/input_size + np.log(input_size)\n",
    "    final = final / np.log(2.0)\n",
    "    return final, final*np.log(2.0)/np.log(input_size), pyx, first_term, second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB_to_Hex(rgb):\n",
    "\n",
    "    RGB = list(rgb)\n",
    "    color = '#'\n",
    "    for i in RGB:\n",
    "        num = int(i)\n",
    "        color += str(hex(num))[-2:].replace('x', '0').upper()\n",
    "    return color\n",
    "    \n",
    "def generate_colors(N=12,colormap='hsv'):\n",
    "\n",
    "    step = max(int(255/N),1)\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    rgb_list = []\n",
    "    hex_list = []\n",
    "    for i in range(N):\n",
    "        id = step*i # cmap(int)->(r,g,b,a) in 0~1\n",
    "        id = 255 if id>255 else id\n",
    "        rgba_color = cmap(id)\n",
    "        rgb = [int(d*255) for d in rgba_color[:3]]\n",
    "        rgb_list.append(tuple(rgb))\n",
    "        hex_list.append(RGB_to_Hex(rgb))\n",
    "    return rgb_list,hex_list\n",
    "    \n",
    "rgb_list,hex_list = generate_colors(6,'cool')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64\n",
    "torch.manual_seed(50)\n",
    "scale = 1\n",
    "batch_size =100\n",
    "nll = nn.NLLLoss()\n",
    "net = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale, effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "#net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "random_samples = []\n",
    "best_ei = 0\n",
    "best_model = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale, effect_size = markov.size()[0], \n",
    "                            hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "#best_model = best_model.cuda() if use_cuda else best_model\n",
    "for t in range(500001):    \n",
    "    state,state_next = generate_data(batch_size)\n",
    "    predict, latent, latent_p = net(state)\n",
    "    predict = nn.functional.log_softmax(predict, dim=1)\n",
    "    loss = nll(predict, torch.nonzero(state_next)[:,1])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "           \n",
    "    if t % 500 ==0:\n",
    "        ei, sigmas,_,one_hot_ei = test_model(batch_size, markov.size()[0], net,scale,nll)\n",
    "        if ei[0]>best_ei:\n",
    "            best_model.load_state_dict(net.state_dict())\n",
    "            best_ei = ei[0]\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss.item(), ', EI= %.3f' % ei[0],\n",
    "             ', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item(), ',onehot_EI = %.3f' % one_hot_ei[0], \n",
    "              ',onehot_ei = %.3f' % one_hot_ei[1])\n",
    "        \n",
    "        if t % 5000 == 0:\n",
    "            ei, sigmas,_,_ = test_model(batch_size, markov.size()[0], best_model,scale,nll)\n",
    "            print('best model: EI= %.3f' % ei[0],\n",
    "                 ', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item())\n",
    "            xx = torch.sum(state * torch.linspace(0,state.size()[1]-1,state.size()[1], device=device),1)\n",
    "            if latent.size()[1]==1:\n",
    "                yy = latent\n",
    "                if use_cuda:\n",
    "                    xx=xx.cpu()\n",
    "                    yy=yy.cpu()\n",
    "                for i in range(yy.size()[1]):\n",
    "                    plt.plot(xx.data, yy[:,i].data,'.')\n",
    "                plt.xlabel('Learned Latent State')\n",
    "                plt.ylabel('Real Latent State')\n",
    "            else:\n",
    "                lowrank=latent\n",
    "                if latent.size()[1]>2:\n",
    "                    lowrank = torch.pca_lowrank(latent, q=2)[0]\n",
    "                plotx = xx.cpu() if use_cuda else xx\n",
    "                ploty = lowrank.cpu() if use_cuda else lowrank\n",
    "                plt.scatter(ploty.data[:, 0], ploty.data[:, 1], c = plotx,cmap=plt.cm.Spectral)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=torch.linspace(-1,1,100, device=device).unsqueeze(1)\n",
    "#xx=xx.repeat(1, 2)\n",
    "print(xx.size())\n",
    "yy=net.dynamics(xx)+xx\n",
    "#print(yy)\n",
    "yy=yy.cpu() if use_cuda else yy\n",
    "xx=xx.cpu() if use_cuda else xx\n",
    "plt.plot(xx[:,0].data,yy[:,0].data,color='royalblue')\n",
    "plt.xlabel('Latent Macro State at Current Time',fontsize=14)\n",
    "plt.ylabel('Latent Macro State at Next Time',fontsize=14)\n",
    "plt.savefig('Simple_Markov1.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=torch.eye(8)\n",
    "if use_cuda:\n",
    "    state=state.cpu()\n",
    "latent=net.encoding(state).squeeze()\n",
    "print(latent)\n",
    "if use_cuda:\n",
    "    latent=latent.cpu()\n",
    "bools = latent>-0.25\n",
    "xx=np.array(range(8))\n",
    "plt.plot(xx[bools], latent[bools].data, 'o', markersize=4, label='Class 1',color='mediumblue')\n",
    "plt.plot(xx[~bools],latent[~bools].data, 's', markersize=4, label='Class 2',color='orangered')\n",
    "plt.xlabel('Encoded Micro State',fontsize=14)\n",
    "plt.ylabel('Latent Macro State',fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('Simple_Markov2.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-scale experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64\n",
    "torch.manual_seed(50)\n",
    "scale = 8\n",
    "batch_size =100\n",
    "nll = nn.NLLLoss()\n",
    "net = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale, effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "#net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "random_samples = []\n",
    "best_ei = 0\n",
    "best_model = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale, effect_size = markov.size()[0], \n",
    "                            hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "#best_model = best_model.cuda() if use_cuda else best_model\n",
    "for t in range(50001):    \n",
    "    state,state_next = generate_data(batch_size)\n",
    "    predict, latent, latent_p = net(state)\n",
    "    predict = nn.functional.log_softmax(predict, dim=1)\n",
    "    loss = nll(predict, torch.nonzero(state_next)[:,1])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "           \n",
    "    if t % 500 ==0:\n",
    "        ei, sigmas,_,one_hot_ei = test_model(batch_size, markov.size()[0], net,scale,nll)\n",
    "        if ei[0]>best_ei:\n",
    "            best_model.load_state_dict(net.state_dict())\n",
    "            best_ei = ei[0]\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss.item(), ', EI= %.3f' % ei[0],\n",
    "             ', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item(), ',onehot_EI = %.3f' % one_hot_ei[0], \n",
    "              ',onehot_ei = %.3f' % one_hot_ei[1])\n",
    "        \n",
    "        if t % 5000 == 0:\n",
    "            ei, sigmas,_,_ = test_model(batch_size, markov.size()[0], best_model,scale,nll)\n",
    "            print('best model: EI= %.3f' % ei[0],\n",
    "                 ', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "#torch.manual_seed(2050)\n",
    "experiments = 5\n",
    "batch_size =100\n",
    "epochs=50001\n",
    "nll = nn.NLLLoss()\n",
    "\n",
    "err_scale = []\n",
    "ei_scale=[]\n",
    "for scale in [8,7,6,5,4,3,2,1]:\n",
    "    print('*********************',scale,'**************************')\n",
    "    err_experiment=[]\n",
    "    multi_err_experiment=[]\n",
    "    ei_experiment=[]\n",
    "    for experiment in range(experiments):\n",
    "        net = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale, effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False,device=device)\n",
    "        net_m= Renorm_Dynamic(sym_size = markov.size()[0], latent_size = markov.size()[0], effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False,device=device)\n",
    "        #net = net.cuda() if use_cuda else net\n",
    "        #net_m= net_m.cuda() if use_cuda else net\n",
    "        optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "        optimizer_m = torch.optim.Adam([p for p in net_m.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "        best_ei = 0\n",
    "        best_model = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale, effect_size = markov.size()[0], \n",
    "                                    hidden_units = hidden_units, normalized_state = False,device=device)\n",
    "        #best_model = best_model.cuda() if use_cuda else best_model\n",
    "        for t in range(epochs):    \n",
    "            state,state_next = generate_data(batch_size)\n",
    "            predict, latent, latent_p = net(state)\n",
    "            predict_m, latent_m, latent_pm = net_m(state)\n",
    "            predict = nn.functional.log_softmax(predict, dim=1)\n",
    "            predict_m = nn.functional.log_softmax(predict_m, dim=1)\n",
    "            loss = nll(predict, torch.nonzero(state_next)[:,1])\n",
    "            loss_m = nll(predict_m, torch.nonzero(state_next)[:,1])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_m.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            loss_m.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            optimizer_m.step()\n",
    "\n",
    "            if t % 5000 ==0:\n",
    "                ei, sigmas,loss,one_ei = test_model(batch_size, markov.size()[0], net,scale, nll)\n",
    "                ei_m, sigmas_m,loss_m,one_ei_m = test_model(batch_size, markov.size()[0], net_m, markov.size()[0], nll)\n",
    "                if ei[0]>best_ei:\n",
    "                    best_model.load_state_dict(net.state_dict())\n",
    "                    best_ei=ei[0]\n",
    "                print('iter %s:' % t, 'loss = %.3f' % loss, ', EI-EIm= %.3f' % (ei[0]-ei_m[0]),\n",
    "                     ', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean(), \n",
    "                      ',onehot_EI = %.3f' % one_hot_ei[0], ',onehot_ei = %.3f' % one_hot_ei[1])\n",
    "        #test\n",
    "        ei, sigmas,loss,_ = test_model(batch_size*10, markov.size()[0], net, scale, nll) \n",
    "        ei_m, sigmas_m,loss_m,_ = test_model(batch_size*10, markov.size()[0], net, scale, nll)\n",
    "        err_experiment.append(loss)\n",
    "        \n",
    "        #mutual information\n",
    "        ei_experiment.append(ei[0]-ei_m[0])\n",
    "        \n",
    "    err_scale.append(err_experiment)\n",
    "    ei_scale.append(ei_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casual Emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(2,'cool')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n",
    "#Spring data\n",
    "ces=ei_scale\n",
    "\n",
    "scales=torch.Tensor([8,7,6,5,4,3,2,1])\n",
    "means=[]\n",
    "stds=[]\n",
    "\n",
    "for ce in ces:\n",
    "    m=np.mean(ce)\n",
    "    std=np.std(ce)\n",
    "    means.append(m)\n",
    "    stds.append(std)\n",
    "\n",
    "#plt.figure(figsize=(3.92*2,2.66*2), dpi = 80)\n",
    "plt.plot(scales, means,'o')\n",
    "plt.errorbar(scales, means, stds,color=hex_list[0])\n",
    "plt.bar(scales, means, width=0.3, facecolor=hex_list[1], edgecolor='white')\n",
    "plt.xlabel('Scale ($q$)',fontsize=13)\n",
    "plt.ylabel('Casual Emergence (dCE)',fontsize=13)\n",
    "plt.title('Relationship between dCE and Scale(q)',fontsize=14) \n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.savefig('Simple_casual_emergence.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entropy_estimators as ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64\n",
    "torch.manual_seed(50)\n",
    "batch_size =100\n",
    "nll = nn.NLLLoss()\n",
    "scale = [6,5,4,3]\n",
    "net_1 = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale[0], effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "net_2 = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale[1], effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "net_3 = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale[2], effect_size = markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "net_4 = Renorm_Dynamic(sym_size = markov.size()[0], latent_size = scale[3], effect_size =markov.size()[0], \n",
    "                     hidden_units = hidden_units, normalized_state = False, device=device)\n",
    "#net = net.cuda() if use_cuda else net\n",
    "optimizer_1 = torch.optim.Adam([p for p in net_1.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "optimizer_2 = torch.optim.Adam([p for p in net_2.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "optimizer_3 = torch.optim.Adam([p for p in net_3.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "optimizer_4 = torch.optim.Adam([p for p in net_4.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "random_samples = []\n",
    "\n",
    "Isshat_1=[]\n",
    "Isshat_2=[]\n",
    "Isshat_3=[]\n",
    "Isshat_4=[]\n",
    "#Isshat_m=[]\n",
    "I_xt_yt_1s=[]\n",
    "I_xt_yt_2s=[]\n",
    "I_xt_yt_3s=[]\n",
    "I_xt_yt_4s=[]\n",
    "#errs=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(50001): \n",
    "    state,state_next=generate_data(batch_size)\n",
    "    \n",
    "    predict_1, latent_1, latent_p1 = net_1(state)\n",
    "    predict_2, latent_2, latent_p2 = net_2(state)\n",
    "    predict_3, latent_3, latent_p3 = net_3(state)\n",
    "    predict_4, latent_4, latent_p4 = net_4(state)\n",
    "    #predict = nn.functional.log_softmax(predict, dim=1)\n",
    "    loss_1 = nll(predict_1, torch.nonzero(state_next)[:,1])\n",
    "    loss_2 = nll(predict_2, torch.nonzero(state_next)[:,1])\n",
    "    loss_3 = nll(predict_3, torch.nonzero(state_next)[:,1])\n",
    "    loss_4 = nll(predict_4, torch.nonzero(state_next)[:,1])\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_2.zero_grad()\n",
    "    optimizer_3.zero_grad()\n",
    "    optimizer_4.zero_grad()\n",
    "    loss_1.backward(retain_graph=True)\n",
    "    loss_2.backward(retain_graph=True)\n",
    "    loss_3.backward(retain_graph=True)\n",
    "    loss_4.backward(retain_graph=True)\n",
    "    optimizer_1.step()\n",
    "    optimizer_2.step()\n",
    "    optimizer_3.step()\n",
    "    optimizer_4.step()\n",
    "\n",
    "    if t % 500 ==0:\n",
    "        I_xt_xt1hat_1=ee.mi(state.cpu(),predict_1.cpu().detach(),k=9)\n",
    "        I_xt_yt_1=ee.mi(state.cpu(),latent_1.cpu().detach(),k=9)\n",
    "\n",
    "        I_xt_yt_2=ee.mi(state.cpu(),latent_2.cpu().detach(),k=9)\n",
    "        I_xt_xt1hat_2=ee.mi(state.cpu(),predict_2.cpu().detach(),k=9)\n",
    "\n",
    "        I_xt_xt1hat_3=ee.mi(state.cpu(),predict_3.cpu().detach(),k=9)\n",
    "        I_xt_yt_3=ee.mi(state.cpu(),latent_3.cpu().detach(),k=9)\n",
    "\n",
    "        I_xt_xt1hat_4=ee.mi(state.cpu(),predict_4.cpu().detach(),k=9)\n",
    "        I_xt_yt_4=ee.mi(state.cpu(),latent_4.cpu().detach(),k=9)\n",
    "\n",
    "        Isshat_1.append(I_xt_xt1hat_1)\n",
    "        Isshat_2.append(I_xt_xt1hat_2)\n",
    "        Isshat_3.append(I_xt_xt1hat_3)\n",
    "        Isshat_4.append(I_xt_xt1hat_4)\n",
    "        I_xt_yt_1s.append(I_xt_yt_1)\n",
    "        I_xt_yt_2s.append(I_xt_yt_2)\n",
    "        I_xt_yt_3s.append(I_xt_yt_3)\n",
    "        I_xt_yt_4s.append(I_xt_yt_4)\n",
    "\n",
    "        #errs.append(np.std([np.mean(I_yt_yt1),np.mean(I_xt_xt1hat),np.mean(I_xt_xt1)]))\n",
    "        Ts.append(t)\n",
    "        \n",
    "        print(I_xt_xt1hat_4,I_xt_yt_1,I_xt_yt_2,I_xt_yt_3,I_xt_yt_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(interval, windowsize):\n",
    "    window = np.ones(int(windowsize)) / float(windowsize)\n",
    "    re = np.convolve(interval, window, 'same')\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Isshat_4_rol_mean = moving_average(Isshat_4,10)\n",
    "Isshat_3_rol_mean = moving_average(Isshat_3,10)\n",
    "Isshat_2_rol_mean = moving_average(Isshat_2,10)\n",
    "Isshat_1_rol_mean = moving_average(Isshat_1,10)\n",
    "I_xt_yt_1s_rol_mean = moving_average(I_xt_yt_1s,10)\n",
    "I_xt_yt_2s_rol_mean = moving_average(I_xt_yt_2s,10)\n",
    "I_xt_yt_3s_rol_mean = moving_average(I_xt_yt_3s,10)\n",
    "I_xt_yt_4s_rol_mean = moving_average(I_xt_yt_4s,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(12,'cubehelix')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n",
    "#plt.plot(Ts,Isshat_3,label='$I(x_t,\\hat{x}_t)$',color='royalblue',linestyle='-.')\n",
    "#plt.plot(Ts,Isshat_m,label='I(xt,xt_hat_m)')\n",
    "#plt.plot(Ts,I_xt_yt_1s,label='$I(x_t,y_{t}^{q=2})$',color='orange',linestyle=':')\n",
    "#plt.plot(Ts,I_xt_yt_2s,label='$I(x_t,y_{t}^{q=4})$',color='g',linestyle=':')\n",
    "#plt.plot(Ts,I_xt_yt_3s,label='$I(x_t,y_{t}^{q=6})$',color='purple',linestyle=':')\n",
    "#plt.plot(Ts,I_xt_yt_4s,label='$I(x_t,y_{t}^{q=8})$',color='dodgerblue',linestyle=':')\n",
    "plt.plot(Ts[6:95],Isshat_1_rol_mean[6:95],label='$I(x_t,\\hat{x}_t)$',color='forestgreen')\n",
    "plt.plot(Ts[6:95],I_xt_yt_1s_rol_mean[6:95],label='$I(x_t,y_{t}^{q=6})$',color='purple')\n",
    "plt.plot(Ts[6:95],I_xt_yt_2s_rol_mean[6:95],label='$I(x_t,y_{t}^{q=5})$',color='dodgerblue')\n",
    "plt.plot(Ts[6:95],I_xt_yt_3s_rol_mean[6:95],label='$I(x_t,y_{t}^{q=4})$',color='crimson')\n",
    "plt.plot(Ts[6:95],I_xt_yt_4s_rol_mean[6:95],label='$I(x_t,y_{t}^{q=3})$)',color='orange')\n",
    "#plt.plot(Ts,I_xt_xt1hat_ms,label='I(xt,xthat)m')\n",
    "#plt.plot(Ts,errs,label='error')\n",
    "plt.legend(loc='best',fontsize=9)\n",
    "plt.ylim(0.5,3.5)\n",
    "plt.title('Verification for Theorem 6',fontsize=14) \n",
    "plt.xlabel('Iter',fontsize=13)\n",
    "plt.ylabel('Mutual Information',fontsize=13)\n",
    "plt.savefig('Markov_Mutual_Information_rm.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
