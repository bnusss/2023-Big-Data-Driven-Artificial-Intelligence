{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import models\n",
    "from models import Renorm_Dynamic\n",
    "from EI_calculation import approx_ei\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0') if use_cuda else torch.device('cpu')\n",
    "\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Mass-Spring Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step(x, v):\n",
    "    #dx/dt=v\n",
    "    #dv/dt=-x\n",
    "    x_ = x + 0.1 * v\n",
    "    v_ = v - 0.1 * x\n",
    "    return x_, v_\n",
    "def multi_steps(s, steps):\n",
    "    s_hist = s\n",
    "    sn_hist = perturb(s, 0)\n",
    "    for t in range(steps):\n",
    "        s_next = one_step(s[:,0],s[:,1])\n",
    "        s_next = torch.Tensor(s_next).unsqueeze(0)\n",
    "        s_next = s_next.to(device)\n",
    "        s_hist = torch.cat((s_hist, s_next), 0)\n",
    "        rand_next = perturb(s_next, sigma)\n",
    "        sn_hist = torch.cat((sn_hist, rand_next), 0)\n",
    "        s = s_next\n",
    "    return s_hist, sn_hist\n",
    "def perturb(s, sigma):\n",
    "    rand = torch.randn([s.size()[0], 2], device=device) * sigma\n",
    "    #rand2 = torch.randn([s.size()[0], 2]) * sigma\n",
    "    #rand3 = torch.randn([s.size()[0], 2]) * sigma\n",
    "    s1 = s - rand\n",
    "    s2 = s + rand\n",
    "    #s3 = s + rand3\n",
    "    sr = torch.cat((s1, s2), 1)\n",
    "    return sr\n",
    "\n",
    "def generate_data(batch_size, sigma, L):\n",
    "    x = 2 * (torch.rand([batch_size, 1], device=device) - 1/2) * L\n",
    "    v = 2 * (torch.rand([batch_size, 1], device=device) - 1/2) * L\n",
    "    xplus, vplus= one_step(x, v)\n",
    "    s_p = perturb(torch.cat((x,v), 1), sigma)\n",
    "    splus_p = perturb(torch.cat((xplus, vplus), 1), sigma)\n",
    "    return s_p, splus_p, torch.cat((x,v),1),torch.cat((xplus,vplus),1)\n",
    "def test_model(batch_size,net,sigma,L,scale):\n",
    "    batch_size1 = batch_size*10\n",
    "    state, state_next,rs,rsp = generate_data(batch_size,sigma,L)\n",
    "    predict, latent, latent_p = net(state) \n",
    "    #L = int(max(torch.amax(torch.abs(latent.view(-1))),torch.amax(torch.abs(latent_p.view(-1)))).item()*10)\n",
    "\n",
    "    ssp = net.encoding(state_next)\n",
    "    sigmas = torch.sqrt(torch.mean((ssp-latent_p)**2, 0))\n",
    "    #sigmas = torch.relu(net.sigmas)+1e-10\n",
    "    sigmas_matrix = torch.diag(sigmas)\n",
    "    ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(net.dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                   num_samples = 1000, L=100, easy=True, device=device)\n",
    "    return ei,sigmas\n",
    "def calc_ei_loss(latent_p, state_next, net, scale): \n",
    "    ssp = net.encoding(state_next)\n",
    "    prediction = ssp#.detach()\n",
    "    real = latent_p#.detach()\n",
    "    # detach the variables of ssp and latent_p which only optimize the dynamics NN.\n",
    "    sigmas = torch.sqrt(torch.mean((prediction - real)**2, 0))\n",
    "    #sigmas = torch.relu(net.sigmas)+1e-10\n",
    "    sigmas_matrix = torch.diag(sigmas).detach()\n",
    "    ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(net.dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                   num_samples = 1000, L=100, easy=True, device=device)\n",
    "    return ei, sigmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB_to_Hex(rgb):\n",
    "    \"\"\"\n",
    "    RGB\n",
    "    Args:\n",
    "        rgb: tuple\n",
    "\n",
    "    Returns:\n",
    "        color: str\n",
    "    \"\"\"\n",
    "    RGB = list(rgb)\n",
    "    color = '#'\n",
    "    for i in RGB:\n",
    "        num = int(i)\n",
    "        color += str(hex(num))[-2:].replace('x', '0').upper()\n",
    "    return color\n",
    "    \n",
    "def generate_colors(N=12,colormap='hsv'):\n",
    "    step = max(int(255/N),1)\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    rgb_list = []\n",
    "    hex_list = []\n",
    "    for i in range(N):\n",
    "        id = step*i # cmap(int)->(r,g,b,a) in 0~1\n",
    "        id = 255 if id>255 else id\n",
    "        rgba_color = cmap(id)\n",
    "        rgb = [int(d*255) for d in rgba_color[:3]]\n",
    "        rgb_list.append(tuple(rgb))\n",
    "        hex_list.append(RGB_to_Hex(rgb))\n",
    "    return rgb_list,hex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(4,'ocean')\n",
    "print(rgb_list)\n",
    "print(hex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =4\n",
    "sz = 4\n",
    "batch_size =100\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "for t in range(10001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        ei,sigmas=test_model(batch_size, net,sigma,L,scale)\n",
    "        ei_micro.append(ei[0])\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss, ', MAE=%.3f' % mae.item(), \n",
    "              ',dEI= %.3f' % ei[0],', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =2\n",
    "sz = 4\n",
    "batch_size =100\n",
    "MAE = torch.nn.L1Loss()\n",
    "\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_macro=[]\n",
    "for t in range(10001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        ei,sigmas=test_model(batch_size, net,sigma,L,scale)\n",
    "        ei_macro.append(ei[0])\n",
    "        ce=ei[0]-ei_micro[t % 500]\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss, ', MAE=%.3f' % mae.item(), \n",
    "              ',dEI_macro-dEI_micro= %.3f' % ce,', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Real and Predicted Macro-state for NIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size1 = batch_size*10\n",
    "s,sp,sr,srp = generate_data(batch_size1, sigma, L)\n",
    "predict, latent, latent_p = net(s) \n",
    "\n",
    "xy = latent_p #- latent\n",
    "xyp = net.encoding(sp)#srp #- sr\n",
    "xplot = torch.linspace(torch.min(torch.cat((xy,xyp),1)).data,torch.max(torch.cat((xy,xyp),1)).data,100, device=device)\n",
    "if use_cuda:\n",
    "    xplot = xplot.cpu()\n",
    "    xy = xy.cpu()\n",
    "    xyp = xyp.cpu()\n",
    "plt.plot(xy[:,1].data, xyp[:,1].data,'o',markersize='2',label='Velocity',color='crimson')\n",
    "plt.plot(xy[:,0].data, xyp[:,0].data,'s',markersize='2', label='Position',color=hex_list[3])\n",
    "plt.plot(xplot.data, xplot.data, '-',label='y=x',color=hex_list[2])\n",
    "plt.title('The Real and Predicted Macro-state for NIS',fontsize=14) \n",
    "plt.xlabel('Predicted Latent State',fontsize=13)\n",
    "plt.ylabel('Decoded Real Latent State',fontsize=13)\n",
    "plt.legend()\n",
    "plt.savefig('spring_1.svg', dpi=600, format='svg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "xx = latent\n",
    "yy = latent_p-latent\n",
    "if use_cuda:\n",
    "    xx = xx.cpu()\n",
    "    yy = yy.cpu()\n",
    "plt.figure()\n",
    "plt.plot(xx[:,1].data, yy[:,0].data,'o',markersize='2', label = 'predicted $(v, dz / dt)$',color='crimson')\n",
    "plt.plot(xx[:,1].data, 0.1*xx[:,1].data, label = 'real $dz / dt = v$',color=hex_list[0])\n",
    "plt.plot(xx[:,0].data, yy[:,1].data,'s',markersize='2',label ='predicted $(z, dv / dt)$',color=hex_list[3])\n",
    "plt.plot(xx[:,0].data, -0.1*xx[:,0].data, label = 'real $dv / dt = - z$',color=hex_list[2])\n",
    "plt.title('The Real and Predicted Dynamics for NIS',fontsize=14) \n",
    "plt.xlabel('Learned Latent State',fontsize=13)\n",
    "plt.ylabel('$\\Delta$ Learned Latent State/$\\Delta$ t',fontsize=13)\n",
    "plt.legend(fontsize=8)\n",
    "plt.savefig('spring_3.svg', dpi=600, format='svg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "err1 = torch.abs(yy[:,0] - 0.1 * xx[:,1])\n",
    "err2 = torch.abs(yy[:,1] + 0.1 * xx[:,0])\n",
    "err = torch.mean(torch.cat((err1, err2), 0))\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position and Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "#torch.manual_seed(2050)\n",
    "steps = 400\n",
    "z = torch.randn([1, 2], device=device)*L/2 \n",
    "s = perturb(z, sigma)\n",
    "s_hist, z_hist = net.multi_step_prediction(s, steps)\n",
    "if use_cuda:\n",
    "    s_hist = s_hist.cpu()\n",
    "    z_hist = z_hist.cpu()\n",
    "plt.plot(z_hist[:, 0].data, z_hist[:, 1].data, '.',label='Predicted',color=hex_list[0])\n",
    "\n",
    "#plt.plot(s_hist[:, 0].data, s_hist[:, 1].data, '*')\n",
    "#plt.plot(s_hist[:, 2].data, s_hist[:, 3].data, '*')\n",
    "\n",
    "rs_hist, rsn_hist = multi_steps(z, steps)\n",
    "if use_cuda:\n",
    "    rs_hist = rs_hist.cpu()\n",
    "    rsn_hist = rsn_hist.cpu()\n",
    "plt.plot(rs_hist[:, 0].data, rs_hist[:, 1].data, label='Real',color=hex_list[1])\n",
    "#plt.plot(rsn_hist[:, 0].data, rsn_hist[:, 1].data)\n",
    "#plt.plot(rsn_hist[:, 2].data, rsn_hist[:, 3].data)\n",
    "plt.xlabel('Position (z)',fontsize=14)\n",
    "plt.ylabel('Velocity (v)',fontsize=14)\n",
    "plt.legend()\n",
    "#plt.savefig('spring_5.svg', dpi=600, format='svg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "means=torch.mean(torch.abs(rsn_hist-s_hist),1)\n",
    "cums=torch.cumsum(means, 0)\n",
    "plt.semilogy(means.data,color=hex_list[0])\n",
    "plt.semilogy(cums.data/np.linspace(1, steps+1, steps+1),color=hex_list[1])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Errors')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = torch.nn.L1Loss()\n",
    "hidden_size = 64\n",
    "block = lambda: nn.Sequential(nn.Linear(4, hidden_size), nn.LeakyReLU(), nn.Linear(hidden_size, hidden_size), nn.LeakyReLU(), nn.Linear(hidden_size, 4))\n",
    "dynamics_direct = nn.ModuleList([block() for _ in range(1)])\n",
    "dynamics_direct = dynamics_direct.cuda() if use_cuda else dynamics_direct\n",
    "batch_size =100\n",
    "optimizer = torch.optim.Adam(list(dynamics_direct.parameters()), lr=1e-4)\n",
    "for t in range(10001):    \n",
    "    s0,sp,_,_ = generate_data(batch_size, 1, 100)\n",
    "    s=s0\n",
    "    for i in range(len(dynamics_direct)):\n",
    "        s = dynamics_direct[i](s)\n",
    "    sh = s + s0\n",
    "    loss = MAE(sh, sp)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size1 = batch_size*10\n",
    "s,sp,sr,srp = generate_data(batch_size1, sigma, L)\n",
    "s0 = s\n",
    "for i in range(len(dynamics_direct)):\n",
    "    s = dynamics_direct[i](s)\n",
    "prediction = s + s0\n",
    "xy = prediction - s0\n",
    "xyp = srp - sr\n",
    "\n",
    "if use_cuda:\n",
    "    xy = xy.cpu()\n",
    "    xyp = xyp.cpu()\n",
    "    xplot = xplot.cpu()\n",
    "plt.plot(xy[:,1].data, xyp[:,1].data,'o',markersize='2',label='Velocity',color='crimson')\n",
    "plt.plot(xy[:,0].data, xyp[:,0].data,'s',markersize='2', label='Position',color=hex_list[3])\n",
    "plt.plot(xplot.data, xplot.data, '-',label='y=x',color=hex_list[2])\n",
    "plt.title('The Real and Predicted Macro-state for NN',fontsize=14) \n",
    "plt.xlabel('Predicted Latent State',fontsize=13)\n",
    "plt.ylabel('Decoded Real Latent State',fontsize=13)\n",
    "plt.legend()\n",
    "plt.savefig('spring_2.svg', dpi=600, format='svg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "xx = s0\n",
    "yy = prediction - s0\n",
    "if use_cuda:\n",
    "    xx = xx.cpu()\n",
    "    yy = yy.cpu()\n",
    "plt.figure()\n",
    "plt.plot(xx[:,1].data, yy[:,0].data,'o',markersize='2', label = 'predicted $(v, dz / dt)$',color='crimson')\n",
    "plt.plot(xx[:,1].data, 0.1*xx[:,1].data, label = 'real $dz / dt = v$',color=hex_list[0])\n",
    "plt.plot(xx[:,0].data, yy[:,1].data,'s',markersize='2',label ='predicted $(z, dv / dt)$',color=hex_list[3])\n",
    "plt.plot(xx[:,0].data, -0.1*xx[:,0].data, label = 'real $dv / dt = - z$',color=hex_list[2])\n",
    "plt.title('The Real and Predicted Dynamics for NN',fontsize=14) \n",
    "plt.xlabel('Learned Latent State',fontsize=13)\n",
    "plt.ylabel('$\\Delta$ Learned Latent State/$\\Delta$ t',fontsize=13)\n",
    "plt.legend(fontsize=8)\n",
    "plt.savefig('spring_4.svg', dpi=600, format='svg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-scale Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "#torch.manual_seed(2050)\n",
    "experiments = 10\n",
    "batch_size =100\n",
    "epochs=10001\n",
    "MAE = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=4\n",
    "ei_micro=[]\n",
    "for experiment in range(experiments):\n",
    "    net = Renorm_Dynamic(sym_size=4, latent_size = scale, effect_size = 4, hidden_units = hidden_units,\n",
    "                        normalized_state = False, device=device)\n",
    "    net = net.cuda() if use_cuda else net\n",
    "    optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "    batch_size1 = batch_size*10\n",
    "    s,sp,sr,srp = generate_data(batch_size1, sigma, L)\n",
    "    predict, latent, latent_p = net(s) \n",
    "    mae = MAE(net.eff_predict(predict), sp)\n",
    "    avg_mae = mae.mean()\n",
    "    \n",
    "    #mutual information\n",
    "    ssp = net.encoding(sp)\n",
    "    # detach the variables of ssp and latent_p which only optimize the dynamics NN.\n",
    "    sigmas = torch.sqrt(torch.mean((ssp - latent_p)**2, 0))\n",
    "    sigmas_matrix = torch.diag(sigmas)\n",
    "    ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(net.dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                       num_samples = 1000, L=100, easy=True, device=device)\n",
    "    print(ei)\n",
    "    ei_micro.append(ei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_scale = []\n",
    "multi_err_scale = []\n",
    "ei_scale=[]\n",
    "ce_scale=[]\n",
    "for scale in [4,3,2,1]:\n",
    "    print('*********************',scale,'**************************')\n",
    "    err_experiment=[]\n",
    "    multi_err_experiment=[]\n",
    "    ei_experiment=[]\n",
    "    ce_experiment=[]\n",
    "    for experiment in range(experiments):\n",
    "        print('----------',experiment+1,'----------')\n",
    "        #micro\n",
    "        print('micro')\n",
    "        net0 = Renorm_Dynamic(sym_size=4, latent_size = 4, effect_size = 4, hidden_units = hidden_units,\n",
    "                        normalized_state = False, device=device)\n",
    "        net0 = net0.cuda() if use_cuda else net0\n",
    "        optimizer0 = torch.optim.Adam([p for p in net0.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "        for t in range(epochs):    \n",
    "            s,sp,_,_ = generate_data(batch_size, sigma, L)\n",
    "            predict0, latent0, latent_p0 = net0(s)\n",
    "            loss0 = MAE(sp, predict0)\n",
    "\n",
    "            optimizer0.zero_grad()\n",
    "            loss0.backward(retain_graph=True)\n",
    "            optimizer0.step()\n",
    "\n",
    "            if t % 500 == 0:\n",
    "                ei0,sigmas0=test_model(batch_size, net0,sigma,L,4)\n",
    "                #print('effective mutual information:',ei)\n",
    "                print('iter %s:' % t, 'loss = %.3f' % loss0, ', dEI= %.3f' % ei0[0],\n",
    "                     ', eff= %.3f' % ei0[1], ', std = %.3f' % sigmas0.mean().item())\n",
    "        batch_size1 = batch_size*10\n",
    "        s,sp,sr,srp = generate_data(batch_size1, sigma, L)\n",
    "        predict0, latent0, latent_p0 = net0(s) \n",
    "        mae0 = MAE(net0.eff_predict(predict0), sp)\n",
    "        avg_mae0 = mae0.mean()\n",
    "        \n",
    "        #mutual information\n",
    "        ssp0 = net0.encoding(sp)\n",
    "        # detach the variables of ssp and latent_p which only optimize the dynamics NN.\n",
    "        sigmas0 = torch.sqrt(torch.mean((ssp0 - latent_p0)**2, 0))\n",
    "        sigmas_matrix0 = torch.diag(sigmas0)\n",
    "        ei0 = approx_ei(4, 4, sigmas_matrix0.data, lambda x:(net0.dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                        num_samples = 1000, L=100, easy=True, device=device)\n",
    "        print('effective mutual information of micro:',ei0)\n",
    "\n",
    "        #macro\n",
    "        print('macro')\n",
    "        net = Renorm_Dynamic(sym_size=4, latent_size = scale, effect_size = 4, hidden_units = hidden_units,\n",
    "                            normalized_state = False, device=device)\n",
    "        net = net.cuda() if use_cuda else net\n",
    "        optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "        for t in range(epochs):    \n",
    "            s,sp,_,_ = generate_data(batch_size, sigma, L)\n",
    "            predict, latent, latent_p = net(s)\n",
    "            loss = MAE(sp, predict)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % 500 == 0:\n",
    "                ei,sigmas=test_model(batch_size, net,sigma,L,scale)\n",
    "                #print('effective mutual information:',ei)\n",
    "                print('iter %s:' % t, 'loss = %.3f' % loss, ', dEI= %.3f' % ei[0],\n",
    "                     ', eff= %.3f' % ei[1], ', std = %.3f' % sigmas.mean().item())\n",
    "        #test\n",
    "        predict, latent, latent_p = net(s) \n",
    "        mae = MAE(net.eff_predict(predict), sp)\n",
    "        avg_mae = mae.mean()\n",
    "        err_experiment.append(avg_mae.item())\n",
    "        \n",
    "        #mutual information\n",
    "        ssp = net.encoding(sp)\n",
    "        sigmas = torch.sqrt(torch.mean((ssp - latent_p)**2, 0))\n",
    "        sigmas_matrix = torch.diag(sigmas)\n",
    "        # detach the variables of ssp and latent_p which only optimize the dynamics NN.\n",
    "        ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(net.dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                       num_samples = 1000, L=100, easy=True, device=device)\n",
    "        print('effective mutual information:',ei)\n",
    "        ei_experiment.append(ei[0])\n",
    "\n",
    "        ce=ei[0]-ei0[0]\n",
    "        print('casual emergence:',ce)\n",
    "        ce_experiment.append(ce)\n",
    "        \n",
    "        #multi-step test\n",
    "        steps = 500\n",
    "        z = torch.randn([1, 2], device=device)*L/2 \n",
    "        s = perturb(z, sigma)\n",
    "        s_hist, z_hist = net.multi_step_prediction(s, steps)\n",
    "        rs_hist, rsn_hist = multi_steps(z, steps)\n",
    "        means=torch.mean(torch.abs(rsn_hist-s_hist),1)\n",
    "        means=means.cpu() if use_cuda else means\n",
    "        cums=torch.cumsum(means, 0)\n",
    "        multi_err_experiment.append(means.data)\n",
    "    err_scale.append(err_experiment)\n",
    "    multi_err_scale.append(multi_err_experiment)\n",
    "    ei_scale.append(ei_experiment)\n",
    "    ce_scale.append(ce_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales=torch.Tensor([4,3,2,1])\n",
    "means=[]\n",
    "stds=[]\n",
    "for err in ce_scale:\n",
    "    m=np.mean(err)\n",
    "    std=np.std(err)\n",
    "    means.append(m)\n",
    "    stds.append(std)\n",
    "plt.plot(scales, means,'o')\n",
    "plt.errorbar(scales, means, stds)\n",
    "plt.xlabel('Scale ($q$)')\n",
    "plt.ylabel('dEI_macro-dEI_micro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casual Emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(2,'cool')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n",
    "\n",
    "#Spring data\n",
    "ces=ce_scale\n",
    "\n",
    "scales=torch.Tensor([4,3,2,1])\n",
    "means=[]\n",
    "stds=[]\n",
    "\n",
    "for ce in ces:\n",
    "    m=np.mean(ce)\n",
    "    std=np.std(ce)\n",
    "    means.append(m)\n",
    "    stds.append(std)\n",
    "\n",
    "#plt.figure(figsize=(3.92*2,2.66*2), dpi = 80)\n",
    "plt.plot(scales, means,'o')\n",
    "plt.errorbar(scales, means, stds,color=hex_list[0])\n",
    "plt.bar(scales, means, width=0.3, facecolor=hex_list[1], edgecolor='white')\n",
    "plt.title('Relationship between dCE and Scale(q)',fontsize=14) \n",
    "plt.xlabel('Scale ($q$)',fontsize=14)\n",
    "plt.ylabel('Causal Emergence (dCE)',fontsize=14)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.savefig('Spring_casual_emergence.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entropy_estimators as ee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification for Information Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =2\n",
    "sz = 4\n",
    "batch_size =1000\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "\n",
    "Islt=[]\n",
    "Islt1=[]\n",
    "Isshat1=[]\n",
    "Isshat=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(20001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    \n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 50 == 0:\n",
    "        I_yt_xth=ee.mi(predict.cpu().detach(),latent.cpu().detach())\n",
    "        I_yt1_xth=ee.mi(predict.cpu().detach(),latent_p.cpu().detach())\n",
    "        I_xt_yt=ee.mi(s.cpu(),latent.cpu().detach())\n",
    "        I_xt_yt1=ee.mi(s.cpu(),latent_p.cpu().detach())\n",
    "        Isshat1.append(I_xt_yt)\n",
    "        Islt.append(I_yt_xth)\n",
    "        Islt1.append(I_yt1_xth)\n",
    "        Isshat.append(I_xt_yt1)\n",
    "        Ts.append(t)\n",
    "\n",
    "        print(t,I_yt_xth,I_yt1_xth,I_xt_yt,I_xt_yt1)\n",
    "\n",
    "plt.plot(Ts,Islt,label='I(hatxt+1,yt)')\n",
    "plt.plot(Ts,Islt1,label='I(hatxt+1,yt_1)')\n",
    "plt.plot(Ts,Isshat1,label='I(xt,yt)')\n",
    "plt.plot(Ts,Isshat,label='I(xt,yt1)')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ts,Islt,label='$I(\\hat{x}_{t+1},y_t)$',color='crimson')\n",
    "plt.plot(Ts,Islt1,label='$I(\\hat{x}_{t+1},y(t+1))$',color='dodgerblue',linestyle='-.')\n",
    "plt.plot(Ts,Isshat1,label='$I(x_t,y_t)$',color='forestgreen')\n",
    "plt.plot(Ts,Isshat,label='$I(x_t,y(t+1))$',color='darkorange',linestyle='-.')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Verification for Information Bottleneck on NIS',fontsize=14) \n",
    "plt.xlabel('Iter',fontsize=13)\n",
    "plt.ylabel('Mutual Information',fontsize=13)\n",
    "plt.ylim(5.5,8)\n",
    "plt.savefig('Spring_Scale_and_I_yt+1.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification for Theorem 6 with Scale and Mutual Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =4\n",
    "sz = 4\n",
    "batch_size =1000\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "Isst=[]\n",
    "Illt=[]\n",
    "Isshat4=[]\n",
    "Irrt4=[]\n",
    "errs=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(20001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    \n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        I_yt_xt=ee.mi(s.cpu(),latent.cpu().detach())\n",
    "        I_xt_xt1hat=ee.mi(s.cpu(),predict.cpu().detach())\n",
    "        Isshat4.append(I_xt_xt1hat)\n",
    "        Irrt4.append(I_yt_xt)\n",
    "        Ts.append(t)\n",
    "\n",
    "        print(t,I_yt_xt,I_xt_xt1hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =3\n",
    "sz = 4\n",
    "batch_size =1000\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "Isst=[]\n",
    "Illt=[]\n",
    "Isshat3=[]\n",
    "Irrt3=[]\n",
    "errs=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(20001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    \n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        I_yt_xt=ee.mi(s.cpu(),latent.cpu().detach())\n",
    "        I_xt_xt1hat=ee.mi(s.cpu(),predict.cpu().detach())\n",
    "        Isshat3.append(I_xt_xt1hat)\n",
    "        Irrt3.append(I_yt_xt)\n",
    "        Ts.append(t)\n",
    "\n",
    "        print(t,I_yt_xt,I_xt_xt1hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =2\n",
    "sz = 4\n",
    "batch_size =1000\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "Isst=[]\n",
    "Illt=[]\n",
    "Isshat2=[]\n",
    "Irrt2=[]\n",
    "errs=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(20001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    \n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        I_yt_xt=ee.mi(s.cpu(),latent.cpu().detach())\n",
    "        I_xt_xt1hat=ee.mi(s.cpu(),predict.cpu().detach())\n",
    "        Isshat2.append(I_xt_xt1hat)\n",
    "        Irrt2.append(I_yt_xt)\n",
    "        Ts.append(t)\n",
    "\n",
    "        print(t,I_yt_xt,I_xt_xt1hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(4,'cool')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n",
    "\n",
    "plt.plot(Ts[1:41],Irrt4[1:41],label='$I(x_t,y_t^{q=4})$',linewidth=2,color=hex_list[0],linestyle='-.')\n",
    "plt.plot(Ts[1:41],Irrt3[1:41],label='$I(x_t,y_t^{q=3})$',linewidth=2,color=hex_list[1],linestyle='--')\n",
    "plt.plot(Ts[1:41],Irrt2[1:41],label='$I(x_t,y_t^{q=2})$',linewidth=2,color=hex_list[2],linestyle='-.')\n",
    "plt.plot(Ts[1:41],Isshat2[1:41],label='$I(x_t,\\hat{x}_{t+1})$',linewidth=2,color=hex_list[3])\n",
    "plt.legend(loc='best')\n",
    "plt.title('Verification for Theorem 6',fontsize=14) \n",
    "plt.xlabel('Iter',fontsize=13)\n",
    "plt.ylabel('Mutual Information',fontsize=13)\n",
    "plt.ylim(4,9.5)\n",
    "plt.savefig('Spring_Scale_and_I.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification for Theorem 2 when Scale(q)=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =3\n",
    "sz = 4\n",
    "batch_size =1000\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "Isst=[]\n",
    "Illt=[]\n",
    "Isshat=[]\n",
    "Irrt=[]\n",
    "errs=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(50001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    \n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        I_xt_xt1=ee.mi(s.cpu(),sp.cpu())\n",
    "        I_yt_yt1=ee.mi(latent.cpu().detach(), latent_p.cpu().detach(),k=3)\n",
    "        I_xt_xt1hat=ee.mi(s.cpu(),predict.cpu().detach())\n",
    "        Isst.append(I_xt_xt1)\n",
    "        Isshat.append(I_xt_xt1hat)\n",
    "        Irrt.append(I_yt_yt1)\n",
    "        errs.append(np.std([I_yt_yt1,I_xt_xt1hat,I_xt_xt1]))\n",
    "        Ts.append(t)\n",
    "\n",
    "        print(t,I_xt_xt1,I_yt_yt1, I_xt_xt1hat)\n",
    "\n",
    "print(Isst,Isshat,Irrt,errs)\n",
    "plt.plot(Ts,Isst,label='I(xt,xt+1)')\n",
    "plt.plot(Ts,Irrt,label='I(yt,yt+1)')\n",
    "plt.plot(Ts,Isshat,label='I(xt,xthat)')\n",
    "plt.plot(Ts,errs,label='error')\n",
    "plt.ylim(0,9)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Value, Scale(q)=3')\n",
    "plt.savefig('Mutual Information when Scale(q)=3.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(4,'viridis')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n",
    "\n",
    "plt.plot(Ts,Isst,label='$I(x_t,x_{t+1})$',color='forestgreen')\n",
    "plt.plot(Ts,Irrt,label='$I(y_t,y(t+1))$',color='crimson',linestyle='-.')\n",
    "plt.plot(Ts,Isshat,label='$I(x_t,\\hat{x}_{t+1})$',color='dodgerblue',linestyle='--')\n",
    "#plt.plot(Ts,errs,label='error',color=hex_list[3])\n",
    "plt.ylim(0,9)\n",
    "\n",
    "plt.title('Verification for Theorem 2 when Scale(q)=3',fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iter',fontsize=13)\n",
    "plt.ylabel('Mutual Information, Scale(q)=3',fontsize=13)\n",
    "plt.savefig('Spring_Mutual_Information_Scale3.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification for Theorem 2 when Scale(q)=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "hidden_units = 64\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "scale =2\n",
    "sz = 4\n",
    "batch_size =1000\n",
    "MAE = torch.nn.L1Loss()\n",
    "net = Renorm_Dynamic(sym_size = sz, latent_size = scale, effect_size = sz, \n",
    "                     hidden_units = hidden_units, normalized_state=False, device = device)\n",
    "net = net.cuda() if use_cuda else net\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "maes = []\n",
    "ei_micro=[]\n",
    "Isst=[]\n",
    "Illt=[]\n",
    "Isshat=[]\n",
    "Irrt=[]\n",
    "errs=[]\n",
    "Ts=[]\n",
    "\n",
    "for t in range(50001):    \n",
    "    s,sp,rs,rsp = generate_data(batch_size, sigma, L)\n",
    "    predict, latent, latent_p = net(s)\n",
    "    \n",
    "    mae = MAE(sp, predict)\n",
    "    maes.append(mae.item())\n",
    "    loss = mae\n",
    "    #print(loss.item())\n",
    "    #loss = mae\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        I_xt_xt1=ee.mi(s.cpu(),sp.cpu())\n",
    "        I_yt_yt1=ee.mi(latent.cpu().detach(), latent_p.cpu().detach(),k=18)\n",
    "        I_xt_xt1hat=ee.mi(s.cpu(),predict.cpu().detach())\n",
    "        Isst.append(I_xt_xt1)\n",
    "        Isshat.append(I_xt_xt1hat)\n",
    "        Irrt.append(I_yt_yt1)\n",
    "        errs.append(np.std([I_yt_yt1,I_xt_xt1hat,I_xt_xt1]))\n",
    "        Ts.append(t)\n",
    "\n",
    "        print(t,I_xt_xt1,I_yt_yt1, I_xt_xt1hat)\n",
    "\n",
    "print(Isst,Isshat,Irrt,errs)\n",
    "plt.plot(Ts,Isst,label='I(xt,xt+1)')\n",
    "plt.plot(Ts,Irrt,label='I(yt,yt+1)')\n",
    "plt.plot(Ts,Isshat,label='I(xt,xthat)')\n",
    "plt.plot(Ts,errs,label='error')\n",
    "plt.ylim(0,9)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Value, Scale(q)=2')\n",
    "plt.savefig('Mutual Information when Scale(q)=2.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_list,hex_list = generate_colors(4,'viridis')\n",
    "print(rgb_list)\n",
    "print(hex_list)\n",
    "\n",
    "plt.plot(Ts,Isst,label='$I(x_t,x_{t+1})$',color='forestgreen')\n",
    "plt.plot(Ts,Irrt,label='$I(y_t,y(t+1))$',color='crimson',linestyle='-.')\n",
    "plt.plot(Ts,Isshat,label='$I(x_t,\\hat{x}_{t+1})$',color='dodgerblue',linestyle='--')\n",
    "#plt.plot(Ts,errs,label='error',color=hex_list[3])\n",
    "plt.ylim(0,9)\n",
    "plt.title('Verification for Theorem 2 when Scale(q)=2',fontsize=14) \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Iter',fontsize=13)\n",
    "plt.ylabel('Mutual Information, Scale(q)=2',fontsize=13)\n",
    "plt.savefig('Spring_Mutual_Information_Scale2.svg', dpi=600, format='svg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
